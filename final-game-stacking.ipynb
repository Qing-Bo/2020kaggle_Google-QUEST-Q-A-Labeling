{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "OFFLINE = False\n",
    "TEST = False\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"  # sp\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "# from tfdeterminism import patch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig,TFRobertaModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import re\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
    "from tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n",
    "from tensorflow.python.keras import backend_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if OFFLINE == False:\n",
    "    sys.path.insert(0, \"../input/transformers/\")\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig,TFRobertaModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold, RepeatedKFold\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import six\n",
    "import collections\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from math import floor, ceil\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n"
     ]
    }
   ],
   "source": [
    "if OFFLINE:\n",
    "    PATH = \"../input/\"\n",
    "else:\n",
    "    PATH = \"/kaggle/input/google-quest-challenge/\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(PATH + 'train.csv')\n",
    "df_test = pd.read_csv(PATH + 'test.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'\n",
    "    ]\n",
    "\n",
    "\n",
    "q_col = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a_col = ['answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1066: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',\"aren't\": 'are not',\n",
    "    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',\n",
    "    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',\n",
    "    'couldâ€™ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',\n",
    "    'didnâ€™t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',\n",
    "    'doesnâ€™t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',\n",
    "    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',\n",
    "    'howâ€™s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',\n",
    "    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',\n",
    "    'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',\n",
    "    'iâ€™ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',\n",
    "    'letâ€™s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',\n",
    "    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',\n",
    "    'mustâ€™ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',\n",
    "    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',\n",
    "    'shouldâ€™ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',\n",
    "    'theyâ€™re': 'they are','theyâ€™ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',\n",
    "    'wasnâ€™t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',\n",
    "    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',\n",
    "    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',\n",
    "    'whatâ€™ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'wonÂ´t': 'will not','wonâ€™t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',\n",
    "    'wouldnâ€™t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',\n",
    "    'Â´cause': 'because','â€™cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"hereâ€™s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"youâ€™ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"â€˜I\":'I',\n",
    "    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\\\n",
    "    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\\\n",
    "    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\\\n",
    "    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\\\n",
    "    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\\\n",
    "    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',\n",
    "    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',\n",
    "    }\n",
    "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ðŸ˜‰':'wink','ðŸ˜‚':'joy','ðŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
    "\n",
    "special_punc_mappings = {\"â€”\": \"-\", \"â€“\": \"-\", \"_\": \"-\", 'â€': '\"', \"â€³\": '\"', 'â€œ': '\"', 'â€¢': '.', 'âˆ’': '-',\n",
    "                         \"â€™\": \"'\", \"â€˜\": \"'\", \"Â´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ØŒ':'','â€ž':'',\n",
    "                         'â€¦': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
    "                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
    "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
    "                      ' u r ': ' you are ', ' u ': ' you ', 'æ“ä½ å¦ˆ': 'fuck your mother', 'e.g.': 'for example',\n",
    "                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
    "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
    "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
    "                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
    "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
    "                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n",
    "                      }\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£',\n",
    "    'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',\n",
    "    'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€',\n",
    "    'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾',\n",
    "    'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼',\n",
    "    'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²',\n",
    "    'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»',\n",
    "    'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜',\n",
    "    'Â¹', 'â‰¤', 'â€¡', 'âˆš', 'Â«', 'Â»', 'Â´', 'Âº', 'Â¾', 'Â¡', 'Â§', 'Â£', 'â‚¤']\n",
    "bad_case_words = {'nationalpost':'national post','businessinsider':'business insider','jewprofits': 'jew profits', 'QMAS': 'Quality Migrant Admission Scheme', 'casterating': 'castrating',\n",
    "                  'Kashmiristan': 'Kashmir', 'CareOnGo': 'India first and largest Online distributor of medicines',\n",
    "                  'Setya Novanto': 'a former Indonesian politician', 'TestoUltra': 'male sexual enhancement supplement',\n",
    "                  'rammayana': 'ramayana', 'Badaganadu': 'Brahmin community that mainly reside in Karnataka',\n",
    "                  'bitcjes': 'bitches', 'mastubrate': 'masturbate', 'FranÃ§ais': 'France',\n",
    "                  'Adsresses': 'address', 'flemmings': 'flemming', 'intermate': 'inter mating', 'feminisam': 'feminism',\n",
    "                  'cuckholdry': 'cuckold', 'Niggor': 'black hip-hop and electronic artist', 'narcsissist': 'narcissist',\n",
    "                  'Genderfluid': 'Gender fluid', ' Im ': ' I am ', ' dont ': ' do not ', 'Qoura': 'Quora',\n",
    "                  'ethethnicitesnicites': 'ethnicity', 'Namit Bathla': 'Content Writer', 'What sApp': 'WhatsApp',\n",
    "                  'FÃ¼hrer': 'Fuhrer', 'covfefe': 'coverage', 'accedentitly': 'accidentally', 'Cuckerberg': 'Zuckerberg',\n",
    "                  'transtrenders': 'incredibly disrespectful to real transgender people',\n",
    "                  'frozen tamod': 'Pornographic website', 'hindians': 'North Indian', 'hindian': 'North Indian',\n",
    "                  'celibatess': 'celibates', 'Trimp': 'Trump', 'wanket': 'wanker', 'wouldd': 'would',\n",
    "                  'arragent': 'arrogant', 'Ra - apist': 'rapist', 'idoot': 'idiot', 'gangstalkers': 'gangs talkers',\n",
    "                  'toastsexual': 'toast sexual', 'inapropriately': 'inappropriately', 'dumbassess': 'dumbass',\n",
    "                  'germanized': 'become german', 'helisexual': 'sexual', 'regilious': 'religious',\n",
    "                  'timetraveller': 'time traveller', 'darkwebcrawler': 'dark webcrawler', 'routez': 'route',\n",
    "                  'trumpians': 'Trump supporters','Trumpster':'trumpeters', 'irreputable': 'reputation', 'serieusly': 'seriously',\n",
    "                  'anti cipation': 'anticipation', 'microaggression': 'micro aggression', 'Afircans': 'Africans',\n",
    "                  'microapologize': 'micro apologize', 'Vishnus': 'Vishnu', 'excritment': 'excitement',\n",
    "                  'disagreemen': 'disagreement', 'gujratis': 'gujarati', 'gujaratis': 'gujarati',\n",
    "                  'ugggggggllly': 'ugly',\n",
    "                  'Germanity': 'German', 'SoyBoys': 'cuck men lacking masculine characteristics',\n",
    "                  'Ð½': 'h', 'Ð¼': 'm', 'Ñ•': 's', 'Ñ‚': 't', 'Ð²': 'b', 'Ï…': 'u', 'Î¹': 'i',\n",
    "                  'genetilia': 'genitalia', 'r - apist': 'rapist', 'Borokabama': 'Barack Obama',\n",
    "                  'arectifier': 'rectifier', 'pettypotus': 'petty potus', 'magibabble': 'magi babble',\n",
    "                  'nothinking': 'thinking', 'centimiters': 'centimeters', 'saffronized': 'India, politics, derogatory',\n",
    "                  'saffronize': 'India, politics, derogatory', ' incect ': ' insect ', 'weenus': 'elbow skin',\n",
    "                  'Pakistainies': 'Pakistanis', 'goodspeaks': 'good speaks', 'inpregnated': 'in pregnant',\n",
    "                  'rapefilms': 'rape films', 'rapiest': 'rapist', 'hatrednesss': 'hatred',\n",
    "                  'heightism': 'height discrimination', 'getmy': 'get my', 'onsocial': 'on social',\n",
    "                  'worstplatform': 'worst platform', 'platfrom': 'platform', 'instagate': 'instigate',\n",
    "                  'Loy Machedeo': 'person', ' dsire ': ' desire ', 'iservant': 'servant', 'intelliegent': 'intelligent',\n",
    "                  'WW 1': ' WW1 ', 'WW 2': ' WW2 ', 'ww 1': ' WW1 ', 'ww 2': ' WW2 ',\n",
    "                  'keralapeoples': 'kerala peoples', 'trumpervotes': 'trumper votes', 'fucktrumpet': 'fuck trumpet',\n",
    "                  'likebJaish': 'like bJaish', 'likemy': 'like my', 'Howlikely': 'How likely',\n",
    "                  'disagreementts': 'disagreements', 'disagreementt': 'disagreement',\n",
    "                  'meninist': \"male chauvinism\", 'feminists': 'feminism supporters', 'Ghumendra': 'Bhupendra',\n",
    "                  'emellishments': 'embellishments',\n",
    "                  'settelemen': 'settlement',\n",
    "                  'Richmencupid': 'rich men dating website', 'richmencupid': 'rich men dating website',\n",
    "                  'Gaudry - Schost': '', 'ladymen': 'ladyboy', 'hasserment': 'Harassment',\n",
    "                  'instrumentalizing': 'instrument', 'darskin': 'dark skin', 'balckwemen': 'balck women',\n",
    "                  'recommendor': 'recommender', 'wowmen': 'women', 'expertthink': 'expert think',\n",
    "                  'whitesplaining': 'white splaining', 'Inquoraing': 'inquiring', 'whilemany': 'while many',\n",
    "                  'manyother': 'many other', 'involvedinthe': 'involved in the', 'slavetrade': 'slave trade',\n",
    "                  'aswell': 'as well', 'fewshowanyRemorse': 'few show any Remorse', 'trageting': 'targeting',\n",
    "                  'getile': 'gentile', 'Gujjus': 'derogatory Gujarati', 'judisciously': 'judiciously',\n",
    "                  'Hue Mungus': 'feminist bait', 'Hugh Mungus': 'feminist bait', 'Hindustanis': '',\n",
    "                  'Virushka': 'Great Relationships Couple', 'exclusinary': 'exclusionary', 'himdus': 'hindus',\n",
    "                  'Milo Yianopolous': 'a British polemicist', 'hidusim': 'hinduism',\n",
    "                  'holocaustable': 'holocaust', 'evangilitacal': 'evangelical', 'Busscas': 'Buscas',\n",
    "                  'holocaustal': 'holocaust', 'incestious': 'incestuous', 'Tennesseus': 'Tennessee',\n",
    "                  'GusDur': 'Gus Dur',\n",
    "                  'RPatah - Tan Eng Hwan': 'Silsilah', 'Reinfectus': 'reinfect', 'pharisaistic': 'pharisaism',\n",
    "                  'nuslims': 'Muslims', 'taskus': '', 'musims': 'Muslims',\n",
    "                  'Musevi': 'the independence of Mexico', ' racious ': 'discrimination expression of racism',\n",
    "                  'Muslimophobia': 'Muslim phobia', 'justyfied': 'justified', 'holocause': 'holocaust',\n",
    "                  'musilim': 'Muslim', 'misandrous': 'misandry', 'glrous': 'glorious', 'desemated': 'decimated',\n",
    "                  'votebanks': 'vote banks', 'Parkistan': 'Pakistan', 'Eurooe': 'Europe', 'animlaistic': 'animalistic',\n",
    "                  'Asiasoid': 'Asian', 'Congoid': 'Congolese', 'inheritantly': 'inherently',\n",
    "                  'Asianisation': 'Becoming Asia',\n",
    "                  'Russosphere': 'russia sphere of influence', 'exMuslims': 'Ex-Muslims',\n",
    "                  'discriminatein': 'discrimination', ' hinus ': ' hindus ', 'Nibirus': 'Nibiru',\n",
    "                  'habius - corpus': 'habeas corpus', 'prentious': 'pretentious', 'Sussia': 'ancient Jewish village',\n",
    "                  'moustachess': 'moustaches', 'Russions': 'Russians', 'Yuguslavia': 'Yugoslavia',\n",
    "                  'atrocitties': 'atrocities', 'Muslimophobe': 'Muslim phobic', 'fallicious': 'fallacious',\n",
    "                  'recussed': 'recursed', '@ usafmonitor': '', 'lustfly': 'lustful', 'canMuslims': 'can Muslims',\n",
    "                  'journalust': 'journalist', 'digustingly': 'disgustingly', 'harasing': 'harassing',\n",
    "                  'greatuncle': 'great uncle', 'Drumpf': 'Trump', 'rejectes': 'rejected', 'polyagamous': 'polygamous',\n",
    "                  'Mushlims': 'Muslims', 'accusition': 'accusation', 'geniusses': 'geniuses',\n",
    "                  'moustachesomething': 'moustache something', 'heineous': 'heinous',\n",
    "                  'Sapiosexuals': 'sapiosexual', 'sapiosexuals': 'sapiosexual', 'Sapiosexual': 'sapiosexual',\n",
    "                  'sapiosexual': 'Sexually attracted to intelligence', 'pansexuals': 'pansexual',\n",
    "                  'autosexual': 'auto sexual', 'sexualSlutty': 'sexual Slutty', 'hetorosexuality': 'hetoro sexuality',\n",
    "                  'chinesese': 'chinese', 'pizza gate': 'debunked conspiracy theory',\n",
    "                  'countryless': 'Having no country',\n",
    "                  'muslimare': 'Muslim are', 'iPhoneX': 'iPhone', 'lionese': 'lioness', 'marionettist': 'Marionettes',\n",
    "                  'demonetize': 'demonetized', 'eneyone': 'anyone', 'Karonese': 'Karo people Indonesia',\n",
    "                  'minderheid': 'minder worse', 'mainstreamly': 'mainstream', 'contraproductive': 'contra productive',\n",
    "                  'diffenky': 'differently', 'abandined': 'abandoned', 'p0 rnstars': 'pornstars',\n",
    "                  'overproud': 'over proud',\n",
    "                  'cheekboned': 'cheek boned', 'heriones': 'heroines', 'eventhogh': 'even though',\n",
    "                  'americanmedicalassoc': 'american medical assoc', 'feelwhen': 'feel when', 'Hhhow': 'how',\n",
    "                  'reallySemites': 'really Semites', 'gamergaye': 'gamersgate', 'manspreading': 'man spreading',\n",
    "                  'thammana': 'Tamannaah Bhatia', 'dogmans': 'dogmas', 'managementskills': 'management skills',\n",
    "                  'mangoliod': 'mongoloid', 'geerymandered': 'gerrymandered', 'mandateing': 'man dateing',\n",
    "                  'Romanium': 'Romanum',\n",
    "                  'mailwoman': 'mail woman', 'humancoalition': 'human coalition',\n",
    "                  'manipullate': 'manipulate', 'everyo0 ne': 'everyone', 'takeove': 'takeover',\n",
    "                  'Nonchristians': 'Non Christians', 'goverenments': 'governments', 'govrment': 'government',\n",
    "                  'polygomists': 'polygamists', 'Demogorgan': 'Demogorgon', 'maralago': 'Mar-a-Lago',\n",
    "                  'antibigots': 'anti bigots', 'gouing': 'going', 'muzaffarbad': 'muzaffarabad',\n",
    "                  'suchvstupid': 'such stupid', 'apartheidisrael': 'apartheid israel', \n",
    "                  'personaltiles': 'personal titles', 'lawyergirlfriend': 'lawyer girl friend',\n",
    "                  'northestern': 'northwestern', 'yeardold': 'years old', 'masskiller': 'mass killer',\n",
    "                  'southeners': 'southerners', 'Unitedstatesian': 'United states',\n",
    "\n",
    "                  'peoplekind': 'people kind', 'peoplelike': 'people like', 'countrypeople': 'country people',\n",
    "                  'shitpeople': 'shit people', 'trumpology': 'trump ology', 'trumpites': 'Trump supporters',\n",
    "                  'trumplies': 'trump lies', 'donaldtrumping': 'donald trumping', 'trumpdating': 'trump dating',\n",
    "                  'trumpsters': 'trumpeters','Trumpers':'president trump', 'ciswomen': 'cis women', 'womenizer': 'womanizer',\n",
    "                  'pregnantwomen': 'pregnant women', 'autoliker': 'auto liker', 'smelllike': 'smell like',\n",
    "                  'autolikers': 'auto likers', 'religiouslike': 'religious like', 'likemail': 'like mail',\n",
    "                  'fislike': 'dislike', 'sneakerlike': 'sneaker like', 'likeâ¬‡': 'like',\n",
    "                  'likelovequotes': 'like lovequotes', 'likelogo': 'like logo', 'sexlike': 'sex like',\n",
    "                  'Whatwould': 'What would', 'Howwould': 'How would', 'manwould': 'man would',\n",
    "                  'exservicemen': 'ex servicemen', 'femenism': 'feminism', 'devopment': 'development',\n",
    "                  'doccuments': 'documents', 'supplementplatform': 'supplement platform', 'mendatory': 'mandatory',\n",
    "                  'moviments': 'movements', 'Kremenchuh': 'Kremenchug', 'docuements': 'documents',\n",
    "                  'determenism': 'determinism', 'envisionment': 'envision ment',\n",
    "                  'tricompartmental': 'tri compartmental', 'AddMovement': 'Add Movement',\n",
    "                  'mentionong': 'mentioning', 'Whichtreatment': 'Which treatment', 'repyament': 'repayment',\n",
    "                  'insemenated': 'inseminated', 'inverstment': 'investment',\n",
    "                  'managemental': 'manage mental', 'Inviromental': 'Environmental', 'menstrution': 'menstruation',\n",
    "                  'indtrument': 'instrument', 'mentenance': 'maintenance', 'fermentqtion': 'fermentation',\n",
    "                  'achivenment': 'achievement', 'mismanagements': 'mis managements', 'requriment': 'requirement',\n",
    "                  'denomenator': 'denominator', 'drparment': 'department', 'acumens': 'acumen s',\n",
    "                  'celemente': 'Clemente', 'manajement': 'management', 'govermenent': 'government',\n",
    "                  'accomplishmments': 'accomplishments', 'rendementry': 'rendement ry',\n",
    "                  'repariments': 'departments', 'menstrute': 'menstruate', 'determenistic': 'deterministic',\n",
    "                  'resigment': 'resignment', 'selfpayment': 'self payment', 'imrpovement': 'improvement',\n",
    "                  'enivironment': 'environment', 'compartmentley': 'compartment',\n",
    "                  'augumented': 'augmented', 'parmenent': 'permanent', 'dealignment': 'de alignment',\n",
    "                  'develepoments': 'developments', 'menstrated': 'menstruated', 'phnomenon': 'phenomenon',\n",
    "                  'Employmment': 'Employment', 'dimensionalise': 'dimensional ise', 'menigioma': 'meningioma',\n",
    "                  'recrument': 'recrement', 'Promenient': 'Provenient', 'gonverment': 'government',\n",
    "                  'statemment': 'statement', 'recuirement': 'requirement', 'invetsment': 'investment',\n",
    "                  'parilment': 'parchment', 'parmently': 'patiently', 'agreementindia': 'agreement india',\n",
    "                  'menifesto': 'manifesto', 'accomplsihments': 'accomplishments', 'disangagement': 'disengagement',\n",
    "                  'aevelopment': 'development', 'procument': 'procumbent', 'harashment': 'harassment',\n",
    "                  'Tiannanmen': 'Tiananmen', 'commensalisms': 'commensal isms', 'devlelpment': 'development',\n",
    "                  'dimensons': 'dimensions', 'recruitment2017': 'recruitment 2017', 'polishment': 'pol ishment',\n",
    "                  'CommentSafe': 'Comment Safe', 'meausrements': 'measurements', 'geomentrical': 'geometrical',\n",
    "                  'undervelopment': 'undevelopment', 'mensurational': 'mensuration al', 'fanmenow': 'fan menow',\n",
    "                  'permenganate': 'permanganate', 'bussinessmen': 'businessmen',\n",
    "                  'supertournaments': 'super tournaments', 'permanmently': 'permanently',\n",
    "                  'lamenectomy': 'lamnectomy', 'assignmentcanyon': 'assignment canyon', 'adgestment': 'adjustment',\n",
    "                  'mentalized': 'metalized', 'docyments': 'documents', 'requairment': 'requirement',\n",
    "                  'batsmencould': 'batsmen could', 'argumentetc': 'argument etc', 'enjoiment': 'enjoyment',\n",
    "                  'invement': 'movement', 'accompliushments': 'accomplishments', 'regements': 'regiments',\n",
    "                  'departmentHow': 'department How', 'Aremenian': 'Armenian', 'amenclinics': 'amen clinics',\n",
    "                  'nonfermented': 'non fermented', 'Instumentation': 'Instrumentation', 'mentalitiy': 'mentality',\n",
    "                  ' govermen ': 'goverment', 'underdevelopement': 'under developement', 'parlimentry': 'parliamentary',\n",
    "                  'indemenity': 'indemnity', 'Inatrumentation': 'Instrumentation', 'menedatory': 'mandatory',\n",
    "                  'mentiri': 'entire', 'accomploshments': 'accomplishments', 'instrumention': 'instrument ion',\n",
    "                  'afvertisements': 'advertisements', 'parlementarian': 'parlement arian',\n",
    "                  'entitlments': 'entitlements', 'endrosment': 'endorsement', 'improment': 'impriment',\n",
    "                  'archaemenid': 'Achaemenid', 'replecement': 'replacement', 'placdment': 'placement',\n",
    "                  'femenise': 'feminise', 'envinment': 'environment', 'AmenityCompany': 'Amenity Company',\n",
    "                  'increaments': 'increments', 'accomplihsments': 'accomplishments',\n",
    "                  'manygovernment': 'many government', 'panishments': 'punishments', 'elinment': 'eloinment',\n",
    "                  'mendalin': 'mend alin', 'farmention': 'farm ention', 'preincrement': 'pre increment',\n",
    "                  'postincrement': 'post increment', 'achviements': 'achievements', 'menditory': 'mandatory',\n",
    "                  'Emouluments': 'Emoluments', 'Stonemen': 'Stone men', 'menmium': 'medium',\n",
    "                  'entaglement': 'entanglement', 'integumen': 'integument', 'harassument': 'harassment',\n",
    "                  'retairment': 'retainment', 'enviorement': 'environment', 'tormentous': 'torment ous',\n",
    "                  'confiment': 'confident', 'Enchroachment': 'Encroachment', 'prelimenary': 'preliminary',\n",
    "                  'fudamental': 'fundamental', 'instrumenot': 'instrument', 'icrement': 'increment',\n",
    "                  'prodimently': 'prominently', 'meniss': 'menise', 'Whoimplemented': 'Who implemented',\n",
    "                  'Representment': 'Rep resentment', 'StartFragment': 'Start Fragment',\n",
    "                  'EndFragment': 'End Fragment', ' documentarie ': ' documentaries ', 'requriments': 'requirements',\n",
    "                  'constitutionaldevelopment': 'constitutional development', 'parlamentarians': 'parliamentarians',\n",
    "                  'Rumenova': 'Rumen ova', 'argruments': 'arguments', 'findamental': 'fundamental',\n",
    "                  'totalinvestment': 'total investment', 'gevernment': 'government', 'recmommend': 'recommend',\n",
    "                  'appsmoment': 'apps moment', 'menstruual': 'menstrual', 'immplemented': 'implemented',\n",
    "                  'engangement': 'engagement', 'invovement': 'involvement', 'returement': 'retirement',\n",
    "                  'simentaneously': 'simultaneously', 'accompishments': 'accomplishments',\n",
    "                  'menstraution': 'menstruation', 'experimently': 'experiment', 'abdimen': 'abdomen',\n",
    "                  'cemenet': 'cement', 'propelment': 'propel ment', 'unamendable': 'un amendable',\n",
    "                  'employmentnews': 'employment news', 'lawforcement': 'law forcement',\n",
    "                  'menstuating': 'menstruating', 'fevelopment': 'development', 'reglamented': 'reg lamented',\n",
    "                  'imrovment': 'improvement', 'recommening': 'recommending', 'sppliment': 'supplement',\n",
    "                  'measument': 'measurement', 'reimbrusement': 'reimbursement', 'Nutrament': 'Nutriment',\n",
    "                  'puniahment': 'punishment', 'subligamentous': 'sub ligamentous', 'comlementry': 'complementary',\n",
    "                  'reteirement': 'retirement', 'envioronments': 'environments', 'haraasment': 'harassment',\n",
    "                  'USAgovernment': 'USA government', 'Apartmentfinder': 'Apartment finder',\n",
    "                  'encironment': 'environment', 'metacompartment': 'meta compartment',\n",
    "                  'augumentation': 'argumentation', 'dsymenorrhoea': 'dysmenorrhoea',\n",
    "                  'nonabandonment': 'non abandonment', 'annoincement': 'announcement',\n",
    "                  'menberships': 'memberships', 'Gamenights': 'Game nights', 'enliightenment': 'enlightenment',\n",
    "                  'supplymentry': 'supplementary', 'parlamentary': 'parliamentary', 'duramen': 'dura men',\n",
    "                  'hotelmanagement': 'hotel management', 'deartment': 'department',\n",
    "                  'treatmentshelp': 'treatments help', 'attirements': 'attire ments',\n",
    "                  'amendmending': 'amend mending', 'pseudomeningocele': 'pseudo meningocele',\n",
    "                  'intrasegmental': 'intra segmental', 'treatmenent': 'treatment', 'infridgement': 'infringement',\n",
    "                  'infringiment': 'infringement', 'recrecommend': 'rec recommend', 'entartaiment': 'entertainment',\n",
    "                  'inplementing': 'implementing', 'indemendent': 'independent', 'tremendeous': 'tremendous',\n",
    "                  'commencial': 'commercial', 'scomplishments': 'accomplishments', 'Emplement': 'Implement',\n",
    "                  'dimensiondimensions': 'dimension dimensions', 'depolyment': 'deployment',\n",
    "                  'conpartment': 'compartment', 'govnments': 'movements', 'menstrat': 'menstruate',\n",
    "                  'accompplishments': 'accomplishments', 'Enchacement': 'Enchancement',\n",
    "                  'developmenent': 'development', 'emmenagogues': 'emmenagogue', 'aggeement': 'agreement',\n",
    "                  'elementsbond': 'elements bond', 'remenant': 'remnant', 'Manamement': 'Management',\n",
    "                  'Augumented': 'Augmented', 'dimensonless': 'dimensionless',\n",
    "                  'ointmentsointments': 'ointments ointments', 'achiements': 'achievements',\n",
    "                  'recurtment': 'recurrent', 'gouverments': 'governments', 'docoment': 'document',\n",
    "                  'programmingassignments': 'programming assignments', 'menifest': 'manifest',\n",
    "                  'investmentguru': 'investment guru', 'deployements': 'deployments', 'Invetsment': 'Investment',\n",
    "                  'plaement': 'placement', 'Perliament': 'Parliament', 'femenists': 'feminists',\n",
    "                  'ecumencial': 'ecumenical', 'advamcements': 'advancements', 'refundment': 'refund ment',\n",
    "                  'settlementtake': 'settlement take', 'mensrooms': 'mens rooms',\n",
    "                  'productManagement': 'product Management', 'armenains': 'armenians',\n",
    "                  'betweenmanagement': 'between management', 'difigurement': 'disfigurement',\n",
    "                  'Armenized': 'Armenize', 'hurrasement': 'hurra sement', 'mamgement': 'management',\n",
    "                  'momuments': 'monuments', 'eauipments': 'equipments', 'managemenet': 'management',\n",
    "                  'treetment': 'treatment', 'webdevelopement': 'web developement', 'supplemenary': 'supplementary',\n",
    "                  'Encironmental': 'Environmental', 'Understandment': 'Understand ment',\n",
    "                  'enrollnment': 'enrollment', 'thinkstrategic': 'think strategic', 'thinkinh': 'thinking',\n",
    "                  'Softthinks': 'Soft thinks', 'underthinking': 'under thinking', 'thinksurvey': 'think survey',\n",
    "                  'whitelash': 'white lash', 'whiteheds': 'whiteheads', 'whitetning': 'whitening',\n",
    "                  'whitegirls': 'white girls', 'whitewalkers': 'white walkers', 'manycountries': 'many countries',\n",
    "                  'accomany': 'accompany', 'fromGermany': 'from Germany', 'manychat': 'many chat',\n",
    "                  'Germanyl': 'Germany l', 'manyness': 'many ness', 'many4': 'many', 'exmuslims': 'ex muslims',\n",
    "                  'digitizeindia': 'digitize india', 'indiarush': 'india rush', 'indiareads': 'india reads',\n",
    "                  'telegraphindia': 'telegraph india', 'Southindia': 'South india', 'Airindia': 'Air india',\n",
    "                  'siliconindia': 'silicon india', 'airindia': 'air india', 'indianleaders': 'indian leaders',\n",
    "                  'fundsindia': 'funds india', 'indianarmy': 'indian army', 'Technoindia': 'Techno india',\n",
    "                  'Betterindia': 'Better india', 'capesindia': 'capes india', 'Rigetti': 'Ligetti',\n",
    "                  'vegetablr': 'vegetable', 'get90': 'get', 'Magetta': 'Maretta', 'nagetive': 'native',\n",
    "                  'isUnforgettable': 'is Unforgettable', 'get630': 'get 630', 'GadgetPack': 'Gadget Pack',\n",
    "                  'Languagetool': 'Language tool', 'bugdget': 'budget', 'africaget': 'africa get',\n",
    "                  'ABnegetive': 'Abnegative', 'orangetheory': 'orange theory', 'getsmuggled': 'get smuggled',\n",
    "                  'avegeta': 'ave geta', 'gettubg': 'getting', 'gadgetsnow': 'gadgets now',\n",
    "                  'surgetank': 'surge tank', 'gadagets': 'gadgets', 'getallparts': 'get allparts',\n",
    "                  'messenget': 'messenger', 'vegetarean': 'vegetarian', 'get1000': 'get 1000',\n",
    "                  'getfinancing': 'get financing', 'getdrip': 'get drip', 'AdsTargets': 'Ads Targets',\n",
    "                  'tgethr': 'together', 'vegetaries': 'vegetables', 'forgetfulnes': 'forgetfulness',\n",
    "                  'fisgeting': 'fidgeting', 'BudgetAir': 'Budget Air',\n",
    "                  'getDepersonalization': 'get Depersonalization', 'negetively': 'negatively',\n",
    "                  'gettibg': 'getting', 'nauget': 'naught', 'Bugetti': 'Bugatti', 'plagetum': 'plage tum',\n",
    "                  'vegetabale': 'vegetable', 'changetip': 'change tip', 'blackwashing': 'black washing',\n",
    "                  'blackpink': 'black pink', 'blackmoney': 'black money',\n",
    "                  'blackmarks': 'black marks', 'blackbeauty': 'black beauty', 'unblacklisted': 'un blacklisted',\n",
    "                  'blackdotes': 'black dotes', 'blackboxing': 'black boxing', 'blackpaper': 'black paper',\n",
    "                  'blackpower': 'black power', 'Latinamericans': 'Latin americans', 'musigma': 'mu sigma',\n",
    "                  'Indominus': 'In dominus', 'usict': 'USSCt', 'indominus': 'in dominus', 'Musigma': 'Mu sigma',\n",
    "                  'plus5': 'plus', 'Russiagate': 'Russia gate', 'russophobic': 'Russophobiac',\n",
    "                  'Marcusean': 'Marcuse an', 'Radijus': 'Radius', 'cobustion': 'combustion',\n",
    "                  'Austrialians': 'Australians', 'mylogenous': 'myogenous', 'Raddus': 'Radius',\n",
    "                  'hetrogenous': 'heterogenous', 'greenhouseeffect': 'greenhouse effect', 'aquous': 'aqueous',\n",
    "                  'Taharrush': 'Tahar rush', 'Senousa': 'Venous', 'diplococcus': 'diplo coccus',\n",
    "                  'CityAirbus': 'City Airbus', 'sponteneously': 'spontaneously', 'trustless': 't rustless',\n",
    "                  'Pushkaram': 'Pushkara m', 'Fusanosuke': 'Fu sanosuke', 'isthmuses': 'isthmus es',\n",
    "                  'lucideus': 'lucidum', 'overjustification': 'over justification', 'Bindusar': 'Bind usar',\n",
    "                  'cousera': 'couler', 'musturbation': 'masturbation', 'infustry': 'industry',\n",
    "                  'Huswifery': 'Huswife ry', 'rombous': 'bombous', 'disengenuously': 'disingenuously',\n",
    "                  'sllybus': 'syllabus', 'celcious': 'delicious', 'cellsius': 'celsius',\n",
    "                  'lethocerus': 'Lethocerus', 'monogmous': 'monogamous', 'Ballyrumpus': 'Bally rumpus',\n",
    "                  'Koushika': 'Koushik a', 'vivipoarous': 'viviparous', 'ludiculous': 'ridiculous',\n",
    "                  'sychronous': 'synchronous', 'industiry': 'industry', 'scuduse': 'scud use',\n",
    "                  'babymust': 'baby must', 'simultqneously': 'simultaneously', 'exust': 'ex ust',\n",
    "                  'notmusing': 'not musing', 'Zamusu': 'Amuse', 'tusaki': 'tu saki', 'Marrakush': 'Marrakesh',\n",
    "                  'justcheaptickets': 'just cheaptickets', 'Ayahusca': 'Ayahausca', 'samousa': 'samosa',\n",
    "                  'Gusenberg': 'Gutenberg', 'illustratuons': 'illustrations', 'extemporeneous': 'extemporaneous',\n",
    "                  'Mathusla': 'Mathusala', 'Confundus': 'Con fundus', 'tusts': 'trusts', 'poisenious': 'poisonous',\n",
    "                  'Mevius': 'Medius', 'inuslating': 'insulating', 'aroused21000': 'aroused 21000',\n",
    "                  'Wenzeslaus': 'Wenceslaus', 'JustinKase': 'Justin Kase', 'purushottampur': 'purushottam pur',\n",
    "                  'citruspay': 'citrus pay', 'secutus': 'sects', 'austentic': 'austenitic',\n",
    "                  'FacePlusPlus': 'Face PlusPlus', 'aysnchronous': 'asynchronous',\n",
    "                  'teamtreehouse': 'team treehouse', 'uncouncious': 'unconscious', 'Priebuss': 'Prie buss',\n",
    "                  'consciousuness': 'consciousness', 'susubsoil': 'su subsoil', 'trimegistus': 'Trismegistus',\n",
    "                  'protopeterous': 'protopterous', 'trustworhty': 'trustworthy', 'ushually': 'usually',\n",
    "                  'industris': 'industries', 'instantneous': 'instantaneous', 'superplus': 'super plus',\n",
    "                  'shrusti': 'shruti', 'hindhus': 'hindus', 'outonomous': 'autonomous', 'reliegious': 'religious',\n",
    "                  'Kousakis': 'Kou sakis', 'reusult': 'result', 'JanusGraph': 'Janus Graph',\n",
    "                  'palusami': 'palus ami', 'mussraff': 'muss raff', 'hukous': 'humous',\n",
    "                  'photoacoustics': 'photo acoustics', 'kushanas': 'kusha nas', 'justdile': 'justice',\n",
    "                  'Massahusetts': 'Massachusetts', 'uspset': 'upset', 'sustinet': 'sustinent',\n",
    "                  'consicious': 'conscious', 'Sadhgurus': 'Sadh gurus', 'hystericus': 'hysteric us',\n",
    "                  'visahouse': 'visa house', 'supersynchronous': 'super synchronous', 'posinous': 'rosinous',\n",
    "                  'Fernbus': 'Fern bus', 'Tiltbrush': 'Tilt brush', 'glueteus': 'gluteus', 'posionus': 'poisons',\n",
    "                  'Freus': 'Frees', 'Zhuchengtyrannus': 'Zhucheng tyrannus', 'savonious': 'sanious',\n",
    "                  'CusJo': 'Cusco', 'congusion': 'confusion', 'dejavus': 'dejavu s', 'uncosious': 'uncopious',\n",
    "                  'previius': 'previous', 'counciousness': 'conciousness', 'lustorus': 'lustrous',\n",
    "                  'sllyabus': 'syllabus', 'mousquitoes': 'mosquitoes', 'Savvius': 'Savvies', 'arceius': 'Arcesius',\n",
    "                  'prejusticed': 'prejudiced', 'requsitioned': 'requisitioned',\n",
    "                  'deindustralization': 'deindustrialization', 'muscleblaze': 'muscle blaze',\n",
    "                  'ConsciousX5': 'conscious', 'nitrogenious': 'nitrogenous', 'mauritious': 'mauritius',\n",
    "                  'rigrously': 'rigorously', 'Yutyrannus': 'Yu tyrannus', 'muscualr': 'muscular',\n",
    "                  'conscoiusness': 'consciousness', 'Causians': 'Crusians', 'WorkFusion': 'Work Fusion',\n",
    "                  'puspak': 'pu spak', 'Inspirus': 'Inspires', 'illiustrations': 'illustrations',\n",
    "                  'Nobushi': 'No bushi', 'theuseof': 'thereof', 'suspicius': 'suspicious', 'Intuous': 'Virtuous',\n",
    "                  'gaushalas': 'gaus halas', 'campusthrough': 'campus through', 'seriousity': 'seriosity',\n",
    "                  'resustence': 'resistence', 'geminatus': 'geminates', 'disquss': 'discuss',\n",
    "                  'nicholus': 'nicholas', 'Husnai': 'Hussar', 'diiscuss': 'discuss', 'diffussion': 'diffusion',\n",
    "                  'phusicist': 'physicist', 'ernomous': 'enormous', 'Khushali': 'Khushal i', 'heitus': 'Leitus',\n",
    "                  'cracksbecause': 'cracks because', 'Nautlius': 'Nautilus', 'trausted': 'trusted',\n",
    "                  'Dardandus': 'Dardanus', 'Megatapirus': 'Mega tapirus', 'clusture': 'culture',\n",
    "                  'vairamuthus': 'vairamuthu s', 'disclousre': 'disclosure',\n",
    "                  'industrilaization': 'industrialization', 'musilms': 'muslims', 'Australia9': 'Australian',\n",
    "                  'causinng': 'causing', 'ibdustries': 'industries', 'searious': 'serious',\n",
    "                  'Coolmuster': 'Cool muster', 'sissyphus': 'sisyphus', ' justificatio ': 'justification',\n",
    "                  'antihindus': 'anti hindus', 'Moduslink': 'Modus link', 'zymogenous': 'zymogen ous',\n",
    "                  'prospeorus': 'prosperous', 'Retrocausality': 'Retro causality', 'FusionGPS': 'Fusion GPS',\n",
    "                  'Mouseflow': 'Mouse flow', 'bootyplus': 'booty plus', 'Itylus': 'I tylus',\n",
    "                  'Olnhausen': 'Olshausen', 'suspeect': 'suspect', 'entusiasta': 'enthusiast',\n",
    "                  'fecetious': 'facetious', 'bussiest': 'fussiest', 'Draconius': 'Draconis',\n",
    "                  'requsite': 'requisite', 'nauseatic': 'nausea tic', 'Brusssels': 'Brussels',\n",
    "                  'repurcussion': 'repercussion', 'Jeisus': 'Jesus', 'philanderous': 'philander ous',\n",
    "                  'muslisms': 'muslims', 'august2017': 'august 2017', 'calccalculus': 'calc calculus',\n",
    "                  'unanonymously': 'un anonymously', 'Imaprtus': 'Impetus', 'carnivorus': 'carnivorous',\n",
    "                  'Corypheus': 'Coryphees', 'austronauts': 'astronauts', 'neucleus': 'nucleus',\n",
    "                  'housepoor': 'house poor', 'rescouses': 'responses', 'Tagushi': 'Tagus hi',\n",
    "                  'hyperfocusing': 'hyper focusing', 'nutriteous': 'nutritious', 'chylus': 'chylous',\n",
    "                  'preussure': 'pressure', 'outfocus': 'out focus', 'Hanfus': 'Hannus', 'Rustyrose': 'Rusty rose',\n",
    "                  'vibhushant': 'vibhushan t', 'conciousnes': 'conciousness', 'Venus25': 'Venus',\n",
    "                  'Sedataious': 'Seditious', 'promuslim': 'pro muslim', 'statusGuru': 'status Guru',\n",
    "                  'yousician': 'musician', 'transgenus': 'trans genus', 'Pushbullet': 'Push bullet',\n",
    "                  'jeesyllabus': 'jee syllabus', 'complusary': 'compulsory', 'Holocoust': 'Holocaust',\n",
    "                  'careerplus': 'career plus', 'Lllustrate': 'Illustrate', 'Musino': 'Musion',\n",
    "                  'Phinneus': 'Phineus', 'usedtoo': 'used too', 'JustBasic': 'Just Basic', 'webmusic': 'web music',\n",
    "                  'TrustKit': 'Trust Kit', 'industrZgies': 'industries', 'rubustness': 'robustness',\n",
    "                  'Missuses': 'Miss uses', 'Musturbation': 'Masturbation', 'bustees': 'bus tees',\n",
    "                  'justyfy': 'justify', 'pegusus': 'pegasus', 'industrybuying': 'industry buying',\n",
    "                  'advantegeous': 'advantageous', 'kotatsus': 'kotatsu s', 'justcreated': 'just created',\n",
    "                  'simultameously': 'simultaneously', 'husoone': 'huso one', 'twiceusing': 'twice using',\n",
    "                  'cetusplay': 'cetus play', 'sqamous': 'squamous', 'claustophobic': 'claustrophobic',\n",
    "                  'Kaushika': 'Kaushik a', 'dioestrus': 'di oestrus', 'Degenerous': 'De generous',\n",
    "                  'neculeus': 'nucleus', 'cutaneously': 'cu taneously', 'Alamotyrannus': 'Alamo tyrannus',\n",
    "                  'Ivanious': 'Avanious', 'arceous': 'araceous', 'Flixbus': 'Flix bus', 'caausing': 'causing',\n",
    "                  'publious': 'Publius', 'Juilus': 'Julius', 'Australianism': 'Australian ism',\n",
    "                  'vetronus': 'verrons', 'nonspontaneous': 'non spontaneous', 'calcalus': 'calculus',\n",
    "                  'commudus': 'Commodus', 'Rheusus': 'Rhesus', 'syallubus': 'syllabus', 'Yousician': 'Musician',\n",
    "                  'qurush': 'qu rush', 'athiust': 'athirst', 'conclusionless': 'conclusion less',\n",
    "                  'usertesting': 'user testing', 'redius': 'radius', 'Austrolia': 'Australia',\n",
    "                  'sllaybus': 'syllabus', 'toponymous': 'top onymous', 'businiss': 'business',\n",
    "                  'hyperthalamus': 'hyper thalamus', 'clause55': 'clause', 'cosicous': 'conscious',\n",
    "                  'Sushena': 'Saphena', 'Luscinus': 'Luscious', 'Prussophile': 'Russophile', 'jeaslous': 'jealous',\n",
    "                  'Austrelia': 'Australia', 'contiguious': 'contiguous',\n",
    "                  'subconsciousnesses': 'sub consciousnesses', ' jusification ': 'justification',\n",
    "                  'dilusion': 'delusion', 'anticoncussive': 'anti concussive', 'disngush': 'disgust',\n",
    "                  'constiously': 'consciously', 'filabustering': 'filibustering', 'GAPbuster': 'GAP buster',\n",
    "                  'insectivourous': 'insectivorous', 'glocuse': 'louse', 'Antritrust': 'Antitrust',\n",
    "                  'thisAustralian': 'this Australian', 'FusionDrive': 'Fusion Drive', 'nuclus': 'nucleus',\n",
    "                  'abussive': 'abusive', 'mustang1': 'mustangs', 'inradius': 'in radius', 'polonious': 'polonius',\n",
    "                  'ofKulbhushan': 'of Kulbhushan', 'homosporous': 'homos porous', 'circumradius': 'circum radius',\n",
    "                  'atlous': 'atrous', 'insustry': 'industry', 'campuswith': 'campus with', 'beacsuse': 'because',\n",
    "                  'concuous': 'conscious', 'nonHindus': 'non Hindus', 'carnivourous': 'carnivorous',\n",
    "                  'tradeplus': 'trade plus', 'Jeruselam': 'Jerusalem',\n",
    "                  'musuclar': 'muscular', 'deangerous': 'dangerous', 'disscused': 'discussed',\n",
    "                  'industdial': 'industrial', 'sallatious': 'fallacious', 'rohmbus': 'rhombus',\n",
    "                  'golusu': 'gol usu', 'Minangkabaus': 'Minangkabau s', 'Mustansiriyah': 'Mustansiriya h',\n",
    "                  'anomymously': 'anonymously', 'abonymously': 'anonymously', 'indrustry': 'industry',\n",
    "                  'Musharrf': 'Musharraf', 'workouses': 'workhouses', 'sponataneously': 'spontaneously',\n",
    "                  'anmuslim': 'an muslim', 'syallbus': 'syllabus', 'presumptuousnes': 'presumptuousness',\n",
    "                  'Thaedus': 'Thaddus', 'industey': 'industry', 'hkust': 'hust', 'Kousseri': 'Kousser i',\n",
    "                  'mousestats': 'mouses tats', 'russiagate': 'russia gate', 'simantaneously': 'simultaneously',\n",
    "                  'Austertana': 'Auster tana', 'infussions': 'infusions', 'coclusion': 'conclusion',\n",
    "                  'sustainabke': 'sustainable', 'tusami': 'tu sami', 'anonimously': 'anonymously',\n",
    "                  'usebase': 'use base', 'balanoglossus': 'Balanoglossus', 'Unglaus': 'Ung laus',\n",
    "                  'ignoramouses': 'ignoramuses', 'snuus': 'snugs', 'reusibility': 'reusability',\n",
    "                  'Straussianism': 'Straussian ism', 'simoultaneously': 'simultaneously',\n",
    "                  'realbonus': 'real bonus', 'nuchakus': 'nunchakus', 'annonimous': 'anonymous',\n",
    "                  'Incestious': 'Incestuous', 'Manuscriptology': 'Manuscript ology', 'difusse': 'diffuse',\n",
    "                  'Pliosaurus': 'Pliosaur us', 'cushelle': 'cush elle', 'Catallus': 'Catullus',\n",
    "                  'MuscleBlaze': 'Muscle Blaze', 'confousing': 'confusing', 'enthusiasmless': 'enthusiasm less',\n",
    "                  'Tetherusd': 'Tethered', 'Josephius': 'Josephus', 'jusrlt': 'just',\n",
    "                  'simutaneusly': 'simultaneously', 'mountaneous': 'mountainous', 'Badonicus': 'Sardonicus',\n",
    "                  'muccus': 'mucous', 'nicus': 'nidus', 'austinlizards': 'austin lizards',\n",
    "                  'errounously': 'erroneously', 'Australua': 'Australia', 'sylaabus': 'syllabus',\n",
    "                  'dusyant': 'distant', 'javadiscussion': 'java discussion', 'megabuses': 'mega buses',\n",
    "                  'danergous': 'dangerous', 'contestious': 'contentious', 'exause': 'excuse',\n",
    "                  'muscluar': 'muscular', 'avacous': 'vacuous', 'Ingenhousz': 'Ingenious',\n",
    "                  'holocausting': 'holocaust ing', 'Pakustan': 'Pakistan', 'purusharthas': 'purushartha',\n",
    "                  'bapus': 'bapu s', 'useul': 'useful', 'pretenious': 'pretentious', 'homogeneus': 'homogeneous',\n",
    "                  'bhlushes': 'blushes', 'Saggittarius': 'Sagittarius', 'sportsusa': 'sports usa',\n",
    "                  'kerataconus': 'keratoconus', 'infrctuous': 'infectuous', 'Anonoymous': 'Anonymous',\n",
    "                  'triphosphorus': 'tri phosphorus', 'ridicjlously': 'ridiculously',\n",
    "                  'worldbusiness': 'world business', 'hollcaust': 'holocaust', 'Dusra': 'Dura',\n",
    "                  'meritious': 'meritorious', 'Sauskes': 'Causes', 'inudustry': 'industry',\n",
    "                  'frustratd': 'frustrate', 'hypotenous': 'hypogenous', 'Dushasana': 'Dush asana',\n",
    "                  'saadus': 'status', 'keratokonus': 'keratoconus', 'Jarrus': 'Harrus', 'neuseous': 'nauseous',\n",
    "                  'simutanously': 'simultaneously', 'diphosphorus': 'di phosphorus', 'sulprus': 'surplus',\n",
    "                  'Hasidus': 'Hasid us', 'suspenive': 'suspensive', 'illlustrator': 'illustrator',\n",
    "                  'userflows': 'user flows', 'intrusivethoughts': 'intrusive thoughts', 'countinous': 'continuous',\n",
    "                  'gpusli': 'gusli', 'Calculus1': 'Calculus', 'bushiri': 'Bushire',\n",
    "                  'torvosaurus': 'Torosaurus', 'chestbusters': 'chest busters', 'Satannus': 'Sat annus',\n",
    "                  'falaxious': 'fallacious', 'obnxious': 'obnoxious', 'tranfusions': 'transfusions',\n",
    "                  'PlayMagnus': 'Play Magnus', 'Epicodus': 'Episodes', 'Hypercubus': 'Hypercubes',\n",
    "                  'Musickers': 'Musick ers', 'programmebecause': 'programme because', 'indiginious': 'indigenous',\n",
    "                  'housban': 'Housman', 'iusso': 'kusso', 'annilingus': 'anilingus', 'Nennus': 'Genius',\n",
    "                  'pussboy': 'puss boy', 'Photoacoustics': 'Photo acoustics', 'Hindusthanis': 'Hindustanis',\n",
    "                  'lndustrial': 'industrial', 'tyrannously': 'tyrannous', 'Susanoomon': 'Susanoo mon',\n",
    "                  'colmbus': 'columbus', 'sussessful': 'successful', 'ousmania': 'ous mania',\n",
    "                  'ilustrating': 'illustrating', 'famousbirthdays': 'famous birthdays',\n",
    "                  'suspectance': 'suspect ance', 'extroneous': 'extraneous', 'teethbrush': 'teeth brush',\n",
    "                  'abcmouse': 'abc mouse', 'degenerous': 'de generous', 'doesGauss': 'does Gauss',\n",
    "                  'insipudus': 'insipidus', 'movielush': 'movie lush', 'Rustichello': 'Rustic hello',\n",
    "                  'Firdausiya': 'Firdausi ya', 'checkusers': 'check users', 'householdware': 'household ware',\n",
    "                  'prosporously': 'prosperously', 'SteLouse': 'Ste Louse', 'obfuscaton': 'obfuscation',\n",
    "                  'amorphus': 'amorph us', 'trustworhy': 'trustworthy', 'celsious': 'cesious',\n",
    "                  'dangorous': 'dangerous', 'anticancerous': 'anti cancerous', 'cousi ': 'cousin ',\n",
    "                  'austroloid': 'australoid', 'fergussion': 'percussion', 'andKyokushin': 'and Kyokushin',\n",
    "                  'cousan': 'cousin', 'Huskystar': 'Hu skystar', 'retrovisus': 'retrovirus', 'becausr': 'because',\n",
    "                  'Jerusalsem': 'Jerusalem', 'motorious': 'notorious', 'industrilised': 'industrialised',\n",
    "                  'powerballsusa': 'powerballs usa', 'monoceious': 'monoecious', 'batteriesplus': 'batteries plus',\n",
    "                  'nonviscuous': 'nonviscous', 'industion': 'induction', 'bussinss': 'bussings',\n",
    "                  'userbags': 'user bags', 'Jlius': 'Julius', 'thausand': 'thousand', 'plustwo': 'plus two',\n",
    "                  'defpush': 'def push', 'subconcussive': 'sub concussive', 'muslium': 'muslim',\n",
    "                  'industrilization': 'industrialization', 'Maurititus': 'Mauritius', 'uslme': 'some',\n",
    "                  'Susgaon': 'Surgeon', 'Pantherous': 'Panther ous', 'antivirius': 'antivirus',\n",
    "                  'Trustclix': 'Trust clix', 'silumtaneously': 'simultaneously', 'Icompus': 'Corpus',\n",
    "                  'atonomous': 'autonomous', 'Reveuse': 'Reve use', 'legumnous': 'leguminous',\n",
    "                  'syllaybus': 'syllabus', 'louspeaker': 'loudspeaker', 'susbtraction': 'substraction',\n",
    "                  'virituous': 'virtuous', 'disastrius': 'disastrous', 'jerussalem': 'jerusalem',\n",
    "                  'Industrailzed': 'Industrialized', 'recusion': 'recushion',\n",
    "                  'simultenously': 'simultaneously',\n",
    "                  'Pulphus': 'Pulpous', 'harbaceous': 'herbaceous', 'phlegmonous': 'phlegmon ous', 'use38': 'use',\n",
    "                  'jusify': 'justify', 'instatanously': 'instantaneously', 'tetramerous': 'tetramer ous',\n",
    "                  'usedvin': 'used vin', 'sagittarious': 'sagittarius', 'mausturbate': 'masturbate',\n",
    "                  'subcautaneous': 'subcutaneous', 'dangergrous': 'dangerous', 'sylabbus': 'syllabus',\n",
    "                  'hetorozygous': 'heterozygous', 'Ignasius': 'Ignacius', 'businessbor': 'business bor',\n",
    "                  'Bhushi': 'Thushi', 'Moussolini': 'Mussolini', 'usucaption': 'usu caption',\n",
    "                  'Customzation': 'Customization', 'cretinously': 'cretinous', 'genuiuses': 'geniuses',\n",
    "                  'Moushmee': 'Mousmee', 'neigous': 'nervous',\n",
    "                  'infrustructre': 'infrastructure', 'Ilusha': 'Ilesha', 'suconciously': 'unconciously',\n",
    "                  'stusy': 'study', 'mustectomy': 'mastectomy', 'Farmhousebistro': 'Farmhouse bistro',\n",
    "                  'instantanous': 'instantaneous', 'JustForex': 'Just Forex', 'Indusyry': 'Industry',\n",
    "                  'mustabating': 'must abating', 'uninstrusive': 'unintrusive', 'customshoes': 'customs hoes',\n",
    "                  'homageneous': 'homogeneous', 'Empericus': 'Imperious', 'demisexuality': 'demi sexuality',\n",
    "                  'transexualism': 'transsexualism', 'sexualises': 'sexualise', 'demisexuals': 'demisexual',\n",
    "                  'sexuly': 'sexily', 'Pornosexuality': 'Porno sexuality', 'sexond': 'second', 'sexxual': 'sexual',\n",
    "                  'asexaul': 'asexual', 'sextactic': 'sex tactic', 'sexualityism': 'sexuality ism',\n",
    "                  'monosexuality': 'mono sexuality', 'intwrsex': 'intersex', 'hypersexualize': 'hyper sexualize',\n",
    "                  'homosexualtiy': 'homosexuality', 'examsexams': 'exams exams', 'sexmates': 'sex mates',\n",
    "                  'sexyjobs': 'sexy jobs', 'sexitest': 'sexiest', 'fraysexual': 'fray sexual',\n",
    "                  'sexsurrogates': 'sex surrogates', 'sexuallly': 'sexually', 'gamersexual': 'gamer sexual',\n",
    "                  'greysexual': 'grey sexual', 'omnisexuality': 'omni sexuality', 'hetereosexual': 'heterosexual',\n",
    "                  'productsexamples': 'products examples', 'sexgods': 'sex gods', 'semisexual': 'semi sexual',\n",
    "                  'homosexulity': 'homosexuality', 'sexeverytime': 'sex everytime', 'neurosexist': 'neuro sexist',\n",
    "                  'worldquant': 'world quant', 'Freshersworld': 'Freshers world', 'smartworld': 'sm artworld',\n",
    "                  'Mistworlds': 'Mist worlds', 'boothworld': 'booth world', 'ecoworld': 'eco world',\n",
    "                  'Ecoworld': 'Eco world', 'underworldly': 'under worldly', 'worldrank': 'world rank',\n",
    "                  'Clearworld': 'Clear world', 'Boothworld': 'Booth world', 'Rimworld': 'Rim world',\n",
    "                  'cryptoworld': 'crypto world', 'machineworld': 'machine world', 'worldwideley': 'worldwide ley',\n",
    "                  'capuletwant': 'capulet want', 'Bhagwanti': 'Bhagwant i', 'Unwanted72': 'Unwanted 72',\n",
    "                  'wantrank': 'want rank',\n",
    "                  'willhappen': 'will happen', 'thateasily': 'that easily',\n",
    "                  'Whatevidence': 'What evidence', 'metaphosphates': 'meta phosphates',\n",
    "                  'exilarchate': 'exilarch ate', 'aulphate': 'sulphate', 'Whateducation': 'What education',\n",
    "                  'persulphates': 'per sulphates', 'disulphate': 'di sulphate', 'picosulphate': 'pico sulphate',\n",
    "                  'tetraosulphate': 'tetrao sulphate', 'prechinese': 'pre chinese',\n",
    "                  'Hellochinese': 'Hello chinese', 'muchdeveloped': 'much developed', 'stomuch': 'stomach',\n",
    "                  'Whatmakes': 'What makes', 'Lensmaker': 'Lens maker', 'eyemake': 'eye make',\n",
    "                  'Techmakers': 'Tech makers', 'cakemaker': 'cake maker', 'makeup411': 'makeup 411',\n",
    "                  'objectmake': 'object make', 'crazymaker': 'crazy maker', 'techmakers': 'tech makers',\n",
    "                  'makedonian': 'macedonian', 'makeschool': 'make school', 'anxietymake': 'anxiety make',\n",
    "                  'makeshifter': 'make shifter', 'countryball': 'country ball', 'Whichcountry': 'Which country',\n",
    "                  'countryHow': 'country How', 'Zenfone': 'Zen fone', 'Electroneum': 'Electro neum',\n",
    "                  'electroneum': 'electro neum', 'Demonetisation': 'demonetization', 'zenfone': 'zen fone',\n",
    "                  'ZenFone': 'Zen Fone', 'onecoin': 'one coin', 'demonetizing': 'demonetized',\n",
    "                  'iphone7': 'iPhone', 'iPhone6': 'iPhone', 'microneedling': 'micro needling', 'iphone6': 'iPhone',\n",
    "                  'Monegasques': 'Monegasque s', 'demonetised': 'demonetized',\n",
    "                  'EveryoneDiesTM': 'EveryoneDies TM', 'teststerone': 'testosterone', 'DoneDone': 'Done Done',\n",
    "                  'papermoney': 'paper money', 'Sasabone': 'Sasa bone', 'Blackphone': 'Black phone',\n",
    "                  'Bonechiller': 'Bone chiller', 'Moneyfront': 'Money front', 'workdone': 'work done',\n",
    "                  'iphoneX': 'iPhone', 'roxycodone': 'r oxycodone',\n",
    "                  'moneycard': 'money card', 'Fantocone': 'Fantocine', 'eletronegativity': 'electronegativity',\n",
    "                  'mellophones': 'mellophone s', 'isotones': 'iso tones', 'donesnt': 'doesnt',\n",
    "                  'thereanyone': 'there anyone', 'electronegativty': 'electronegativity',\n",
    "                  'commissiioned': 'commissioned', 'earvphone': 'earphone', 'condtioners': 'conditioners',\n",
    "                  'demonetistaion': 'demonetization', 'ballonets': 'ballo nets', 'DoneClaim': 'Done Claim',\n",
    "                  'alimoney': 'alimony', 'iodopovidone': 'iodo povidone', 'bonesetters': 'bone setters',\n",
    "                  'componendo': 'compon endo', 'probationees': 'probationers', 'one300': 'one 300',\n",
    "                  'nonelectrolyte': 'non electrolyte', 'ozonedepletion': 'ozone depletion',\n",
    "                  'Stonehart': 'Stone hart', 'Vodafone2': 'Vodafones', 'chaparone': 'chaperone',\n",
    "                  'Noonein': 'Noo nein', 'Frosione': 'Erosion', 'IPhone7': 'Iphone', 'pentanone': 'penta none',\n",
    "                  'poneglyphs': 'pone glyphs', 'cyclohexenone': 'cyclohexanone', 'marlstone': 'marls tone',\n",
    "                  'androneda': 'andromeda', 'iphone8': 'iPhone', 'acidtone': 'acid tone',\n",
    "                  'noneconomically': 'non economically', 'Honeyfund': 'Honey fund', 'germanophone': 'Germanophobe',\n",
    "                  'Democratizationed': 'Democratization ed', 'haoneymoon': 'honeymoon', 'iPhone7': 'iPhone 7',\n",
    "                  'someonewith': 'some onewith', 'Hexanone': 'Hexa none', 'bonespur': 'bones pur',\n",
    "                  'sisterzoned': 'sister zoned', 'HasAnyone': 'Has Anyone',\n",
    "                  'stonepelters': 'stone pelters', 'Chronexia': 'Chronaxia', 'brotherzone': 'brother zone',\n",
    "                  'brotherzoned': 'brother zoned', 'fonecare': 'f onecare', 'nonexsistence': 'nonexistence',\n",
    "                  'conents': 'contents', 'phonecases': 'phone cases', 'Commissionerates': 'Commissioner ates',\n",
    "                  'activemoney': 'active money', 'dingtone': 'ding tone', 'wheatestone': 'wheatstone',\n",
    "                  'chiropractorone': 'chiropractor one', 'heeadphones': 'headphones', 'Maimonedes': 'Maimonides',\n",
    "                  'onepiecedeals': 'onepiece deals', 'oneblade': 'one blade', 'venetioned': 'Venetianed',\n",
    "                  'sunnyleone': 'sunny leone', 'prendisone': 'prednisone', 'Anglosaxophone': 'Anglo saxophone',\n",
    "                  'Blackphones': 'Black phones', 'jionee': 'jinnee', 'chromonema': 'chromo nema',\n",
    "                  'iodoketones': 'iodo ketones', 'demonetizations': 'demonetization', 'aomeone': 'someone',\n",
    "                  'trillonere': 'trillones', 'abandonee': 'abandon',\n",
    "                  'MasterColonel': 'Master Colonel', 'fronend': 'friend', 'Wildstone': 'Wilds tone',\n",
    "                  'patitioned': 'petitioned', 'lonewolfs': 'lone wolfs', 'Spectrastone': 'Spectra stone',\n",
    "                  'dishonerable': 'dishonorable', 'poisiones': 'poisons',\n",
    "                  'condioner': 'conditioner', 'unpermissioned': 'unper missioned', 'friedzone': 'fried zone',\n",
    "                  'umumoney': 'umu money', 'anyonestudied': 'anyone studied', 'dictioneries': 'dictionaries',\n",
    "                  'nosebone': 'nose bone', 'ofVodafone': 'of Vodafone',\n",
    "                  'Yumstone': 'Yum stone', 'oxandrolonesteroid': 'oxandrolone steroid',\n",
    "                  'Mifeprostone': 'Mifepristone', 'pheramones': 'pheromones',\n",
    "                  'sinophone': 'Sinophobe', 'peloponesian': 'peloponnesian', 'michrophone': 'microphone',\n",
    "                  'commissionets': 'commissioners', 'methedone': 'methadone', 'cobditioners': 'conditioners',\n",
    "                  'urotone': 'protone', 'smarthpone': 'smartphone', 'conecTU': 'connect you', 'beloney': 'boloney',\n",
    "                  'comfortzone': 'comfort zone', 'testostersone': 'testosterone', 'camponente': 'component',\n",
    "                  'Idonesia': 'Indonesia', 'dolostones': 'dolostone', 'psiphone': 'psi phone',\n",
    "                  'ceftriazone': 'ceftriaxone', 'feelonely': 'feel onely', 'monetation': 'moderation',\n",
    "                  'activationenergy': 'activation energy', 'moneydriven': 'money driven',\n",
    "                  'staionery': 'stationery', 'zoneflex': 'zone flex', 'moneycash': 'money cash',\n",
    "                  'conectiin': 'connection', 'Wannaone': 'Wanna one',\n",
    "                  'Pictones': 'Pict ones', 'demonentization': 'demonetization',\n",
    "                  'phenonenon': 'phenomenon', 'evenafter': 'even after', 'Sevenfriday': 'Seven friday',\n",
    "                  'Devendale': 'Evendale', 'theeventchronicle': 'the event chronicle',\n",
    "                  'seventysomething': 'seventy something', 'sevenpointed': 'seven pointed',\n",
    "                  'richfeel': 'rich feel', 'overfeel': 'over feel', 'feelingstupid': 'feeling stupid',\n",
    "                  'Photofeeler': 'Photo feeler', 'feelomgs': 'feelings', 'feelinfs': 'feelings',\n",
    "                  'PlayerUnknown': 'Player Unknown', 'Playerunknown': 'Player unknown', 'knowlefge': 'knowledge',\n",
    "                  'knowledgd': 'knowledge', 'knowledeg': 'knowledge', 'knowble': 'Knowle', 'Howknow': 'Howk now',\n",
    "                  'knowledgeWoods': 'knowledge Woods', 'knownprogramming': 'known programming',\n",
    "                  'selfknowledge': 'self knowledge', 'knowldage': 'knowledge', 'knowyouve': 'know youve',\n",
    "                  'aknowlege': 'knowledge', 'Audetteknown': 'Audette known', 'knowlegdeable': 'knowledgeable',\n",
    "                  'trueoutside': 'true outside', 'saynthesize': 'synthesize', 'EssayTyper': 'Essay Typer',\n",
    "                  'meesaya': 'mee saya', 'Rasayanam': 'Rasayan am', 'fanessay': 'fan essay', 'momsays': 'moms ays',\n",
    "                  'sayying': 'saying', 'saydaw': 'say daw', 'Fanessay': 'Fan essay', 'theyreally': 'they really',\n",
    "                  'gayifying': 'gayed up with homosexual love', 'gayke': 'gay Online retailers',\n",
    "                  'Lingayatism': 'Lingayat',\n",
    "                  'macapugay': 'Macaulay', 'jewsplain': 'jews plain',\n",
    "                  'banggood': 'bang good', 'goodfriends': 'good friends',\n",
    "                  'goodfirms': 'good firms', 'Banggood': 'Bang good', 'dogooder': 'do gooder',\n",
    "                  'stillshots': 'stills hots', 'stillsuits': 'still suits', 'panromantic': 'pan romantic',\n",
    "                  'paracommando': 'para commando', 'romantize': 'romanize', 'manupulative': 'manipulative',\n",
    "                  'manjha': 'mania', 'mankrit': 'mank rit',\n",
    "                  'heteroromantic': 'hetero romantic', 'pulmanery': 'pulmonary', 'manpads': 'man pads',\n",
    "                  'supermaneuverable': 'super maneuverable', 'mandatkry': 'mandatory', 'armanents': 'armaments',\n",
    "                  'manipative': 'mancipative', 'himanity': 'humanity', 'maneuever': 'maneuver',\n",
    "                  'Kumarmangalam': 'Kumar mangalam', 'Brahmanwadi': 'Brahman wadi',\n",
    "                  'exserviceman': 'ex serviceman',\n",
    "                  'managewp': 'managed', 'manies': 'many', 'recordermans': 'recorder mans',\n",
    "                  'Feymann': 'Heymann', 'salemmango': 'salem mango', 'manufraturing': 'manufacturing',\n",
    "                  'sreeman': 'freeman', 'tamanaa': 'Tamanac', 'chlamydomanas': 'chlamydomonas',\n",
    "                  'comandant': 'commandant', 'huemanity': 'humanity', 'manaagerial': 'managerial',\n",
    "                  'lithromantics': 'lith romantics',\n",
    "                  'geramans': 'germans', 'Nagamandala': 'Naga mandala', 'humanitariarism': 'humanitarianism',\n",
    "                  'wattman': 'watt man', 'salesmanago': 'salesman ago', 'Washwoman': 'Wash woman',\n",
    "                  'rammandir': 'ram mandir', 'nomanclature': 'nomenclature', 'Haufman': 'Kaufman',\n",
    "                  'prefomance': 'performance', 'ramanunjan': 'Ramanujan', 'Freemansonry': 'Freemasonry',\n",
    "                  'supermaneuverability': 'super maneuverability', 'manstruate': 'menstruate',\n",
    "                  'Tarumanagara': 'Taruma nagara', 'RomanceTale': 'Romance Tale', 'heteromantic': 'hete romantic',\n",
    "                  'terimanals': 'terminals', 'womansplaining': 'wo mansplaining',\n",
    "                  'performancelearning': 'performance learning', 'sociomantic': 'sciomantic',\n",
    "                  'batmanvoice': 'batman voice', 'PerformanceTesting': 'Performance Testing',\n",
    "                  'manorialism': 'manorial ism', 'newscommando': 'news commando',\n",
    "                  'Entwicklungsroman': 'Entwicklungs roman',\n",
    "                  'Kunstlerroman': 'Kunstler roman', 'bodhidharman': 'Bodhidharma', 'Howmaney': 'How maney',\n",
    "                  'manufucturing': 'manufacturing', 'remmaning': 'remaining', 'rangeman': 'range man',\n",
    "                  'mythomaniac': 'mythomania', 'katgmandu': 'katmandu',\n",
    "                  'Superowoman': 'Superwoman', 'Rahmanland': 'Rahman land', 'Dormmanu': 'Dormant',\n",
    "                  'Geftman': 'Gentman', 'manufacturig': 'manufacturing', 'bramanistic': 'Brahmanistic',\n",
    "                  'padmanabhanagar': 'padmanabhan agar', 'homoromantic': 'homo romantic', 'femanists': 'feminists',\n",
    "                  'demihuman': 'demi human', 'manrega': 'Manresa', 'Pasmanda': 'Pas manda',\n",
    "                  'manufacctured': 'manufactured', 'remaninder': 'remainder', 'Marimanga': 'Mari manga',\n",
    "                  'Sloatman': 'Sloat man', 'manlet': 'man let', 'perfoemance': 'performance',\n",
    "                  'mangolian': 'mongolian', 'mangekyu': 'mange kyu', 'mansatory': 'mandatory',\n",
    "                  'managemebt': 'management', 'manufctures': 'manufactures', 'Bramanical': 'Brahmanical',\n",
    "                  'manaufacturing': 'manufacturing', 'Lakhsman': 'Lakhs man', 'Sarumans': 'Sarum ans',\n",
    "                  'mangalasutra': 'mangalsutra', 'Germanised': 'German ised',\n",
    "                  'managersworking': 'managers working', 'cammando': 'commando', 'mandrillaris': 'mandrill aris',\n",
    "                  'Emmanvel': 'Emmarvel', 'manupalation': 'manipulation', 'welcomeromanian': 'welcome romanian',\n",
    "                  'humanfemale': 'human female', 'mankirt': 'mankind', 'Haffmann': 'Hoffmann',\n",
    "                  'Panromantic': 'Pan romantic', 'demantion': 'detention', 'Suparwoman': 'Superwoman',\n",
    "                  'parasuramans': 'parasuram ans', 'sulmann': 'Suilmann', 'Shubman': 'Subman',\n",
    "                  'manspread': 'man spread', 'mandingan': 'Mandingan', 'mandalikalu': 'mandalika lu',\n",
    "                  'manufraturer': 'manufacturer', 'Wedgieman': 'Wedgie man', 'manwues': 'manages',\n",
    "                  'humanzees': 'human zees', 'Steymann': 'Stedmann', 'Jobberman': 'Jobber man',\n",
    "                  'maniquins': 'mani quins', 'biromantical': 'bi romantical', 'Rovman': 'Roman',\n",
    "                  'pyromantic': 'pyro mantic', 'Tastaman': 'Rastaman', 'Spoolman': 'Spool man',\n",
    "                  'Subramaniyan': 'Subramani yan', 'abhimana': 'abhiman a', 'manholding': 'man holding',\n",
    "                  'seviceman': 'serviceman', 'womansplained': 'womans plained', 'manniya': 'mania',\n",
    "                  'Bhraman': 'Braman', 'Laakman': 'Layman', 'mansturbate': 'masturbate',\n",
    "                  'Sulamaniya': 'Sulamani ya', 'demanters': 'decanters', 'postmanare': 'postman are',\n",
    "                  'mannualy': 'annual', 'rstman': 'Rotman', 'permanentjobs': 'permanent jobs',\n",
    "                  'Allmang': 'All mang', 'TradeCommander': 'Trade Commander', 'BasedStickman': 'Based Stickman',\n",
    "                  'Deshabhimani': 'Desha bhimani', 'manslamming': 'mans lamming', 'Brahmanwad': 'Brahman wad',\n",
    "                  'fundemantally': 'fundamentally', 'supplemantary': 'supplementary', 'egomanias': 'ego manias',\n",
    "                  'manvantar': 'Manvantara', 'spymania': 'spy mania', 'mangonada': 'mango nada',\n",
    "                  'manthras': 'mantras', 'Humanpark': 'Human park', 'manhuas': 'mahuas',\n",
    "                  'manterrupting': 'interrupting', 'dermatillomaniac': 'dermatillomania',\n",
    "                  'performancies': 'performances', 'manipulant': 'manipulate',\n",
    "                  'painterman': 'painter man', 'mangalik': 'manglik',\n",
    "                  'Neurosemantics': 'Neuro semantics', 'discrimantion': 'discrimination',\n",
    "                  'Womansplaining': 'feminist', 'mongodump': 'mongo dump', 'roadgods': 'road gods',\n",
    "                  'Oligodendraglioma': 'Oligodendroglioma', 'unrightly': 'un rightly', 'Janewright': 'Jane wright',\n",
    "                  ' righten ': ' tighten ', 'brightiest': 'brightest',\n",
    "                  'frighter': 'fighter', 'righteouness': 'righteousness', 'triangleright': 'triangle right',\n",
    "                  'Brightspace': 'Brights pace', 'techinacal': 'technical', 'chinawares': 'china wares',\n",
    "                  'Vancouever': 'Vancouver', 'cheverlet': 'cheveret', 'deverstion': 'diversion',\n",
    "                  'everbodys': 'everybody', 'Dramafever': 'Drama fever', 'reverificaton': 'reverification',\n",
    "                  'canterlever': 'canter lever', 'keywordseverywhere': 'keywords everywhere',\n",
    "                  'neverunlearned': 'never unlearned', 'everyfirst': 'every first',\n",
    "                  'neverhteless': 'nevertheless', 'clevercoyote': 'clever coyote', 'irrevershible': 'irreversible',\n",
    "                  'achievership': 'achievers hip', 'easedeverything': 'eased everything', 'youbever': 'you bever',\n",
    "                  'everperson': 'ever person', 'everydsy': 'everyday', 'whemever': 'whenever',\n",
    "                  'everyonr': 'everyone', 'severiity': 'severity', 'narracist': 'nar racist',\n",
    "                  'racistly': 'racist', 'takesuch': 'take such', 'mystakenly': 'mistakenly',\n",
    "                  'shouldntake': 'shouldnt take', 'Kalitake': 'Kali take', 'msitake': 'mistake',\n",
    "                  'straitstimes': 'straits times', 'timefram': 'timeframe', 'watchtime': 'watch time',\n",
    "                  'timetraveling': 'timet raveling', 'peactime': 'peacetime', 'timetabe': 'timetable',\n",
    "                  'cooktime': 'cook time', 'blocktime': 'block time', 'timesjobs': 'times jobs',\n",
    "                  'timesence': 'times ence', 'Touchtime': 'Touch time', 'timeloop': 'time loop',\n",
    "                  'subcentimeter': 'sub centimeter', 'timejobs': 'time jobs', 'Guardtime': 'Guard time',\n",
    "                  'realtimepolitics': 'realtime politics', 'loadingtimes': 'loading times',\n",
    "                  'timesnow': '24-hour English news channel in India', 'timesspark': 'times spark',\n",
    "                  'timetravelling': 'timet ravelling',\n",
    "                  'antimeter': 'anti meter', 'timewaste': 'time waste', 'cryptochristians': 'crypto christians',\n",
    "                  'Whatcould': 'What could', 'becomesdouble': 'becomes double', 'deathbecomes': 'death becomes',\n",
    "                  'youbecome': 'you become', 'greenseer': 'people who possess the magical ability',\n",
    "                  'rseearch': 'research', 'homeseek': 'home seek',\n",
    "                  'Greenseer': 'people who possess the magical ability', 'starseeders': 'star seeders',\n",
    "                  'seekingmillionaire': 'seeking millionaire', 'see\\u202c': 'see',\n",
    "                  'seeies': 'series', 'CodeAgon': 'Code Agon',\n",
    "                  'royago': 'royal', 'Dragonkeeper': 'Dragon keeper', 'mcgreggor': 'McGregor',\n",
    "                  'catrgory': 'category', 'Dragonknight': 'Dragon knight', 'Antergos': 'Anteros',\n",
    "                  'togofogo': 'togo fogo', 'mongorestore': 'mongo restore', 'gorgops': 'gorgons',\n",
    "                  'withgoogle': 'with google', 'goundar': 'Gondar', 'algorthmic': 'algorithmic',\n",
    "                  'goatnuts': 'goat nuts', 'vitilgo': 'vitiligo', 'polygony': 'poly gony',\n",
    "                  'digonals': 'diagonals', 'Luxemgourg': 'Luxembourg', 'UCSanDiego': 'UC SanDiego',\n",
    "                  'Ringostat': 'Ringo stat', 'takingoff': 'taking off', 'MongoImport': 'Mongo Import',\n",
    "                  'alggorithms': 'algorithms', 'dragonknight': 'dragon knight', 'negotiatior': 'negotiation',\n",
    "                  'gomovies': 'go movies', 'Withgott': 'Without',\n",
    "                  'categoried': 'categories', 'Stocklogos': 'Stock logos', 'Pedogogical': 'Pedological',\n",
    "                  'Wedugo': 'Wedge', 'golddig': 'gold dig', 'goldengroup': 'golden group',\n",
    "                  'merrigo': 'merligo', 'googlemapsAPI': 'googlemaps API', 'goldmedal': 'gold medal',\n",
    "                  'golemized': 'polemized', 'Caligornia': 'California', 'unergonomic': 'un ergonomic',\n",
    "                  'fAegon': 'wagon', 'vertigos': 'vertigo s', 'trigonomatry': 'trigonometry',\n",
    "                  'hypogonadic': 'hypogonadia', 'Mogolia': 'Mongolia', 'governmaent': 'government',\n",
    "                  'ergotherapy': 'ergo therapy', 'Bogosort': 'Bogo sort', 'goalwise': 'goal wise',\n",
    "                  'alogorithms': 'algorithms', 'MercadoPago': 'Mercado Pago', 'rivigo': 'rivi go',\n",
    "                  'govshutdown': 'gov shutdown', 'gorlfriend': 'girlfriend',\n",
    "                  'stategovt': 'state govt', 'Chickengonia': 'Chicken gonia', 'Yegorovich': 'Yegorov ich',\n",
    "                  'regognitions': 'recognitions', 'gorichen': 'Gori Chen Mountain',\n",
    "                  'goegraphies': 'geographies', 'gothras': 'goth ras', 'belagola': 'bela gola',\n",
    "                  'snapragon': 'snapdragon', 'oogonial': 'oogonia l', 'Amigofoods': 'Amigo foods',\n",
    "                  'Sigorn': 'son of Styr', 'algorithimic': 'algorithmic',\n",
    "                  'innermongolians': 'inner mongolians', 'ArangoDB': 'Arango DB', 'zigolo': 'gigolo',\n",
    "                  'regognized': 'recognized', 'Moongot': 'Moong ot', 'goldquest': 'gold quest',\n",
    "                  'catagorey': 'category', 'got7': 'got', 'jetbingo': 'jet bingo', 'Dragonchain': 'Dragon chain',\n",
    "                  'catwgorized': 'categorized', 'gogoro': 'gogo ro', 'Tobagoans': 'Tobago ans',\n",
    "                  'digonal': 'di gonal', 'algoritmic': 'algorismic', 'dragonflag': 'dragon flag',\n",
    "                  'Indigoflight': 'Indigo flight',\n",
    "                  'governening': 'governing', 'ergosphere': 'ergo sphere',\n",
    "                  'pingo5': 'pingo', 'Montogo': 'montego', 'Rivigo': 'technology-enabled logistics company',\n",
    "                  'Jigolo': 'Gigolo', 'phythagoras': 'pythagoras', 'Mangolian': 'Mongolian',\n",
    "                  'forgottenfaster': 'forgotten faster', 'stargold': 'a Hindi movie channel',\n",
    "                  'googolplexain': 'googolplexian', 'corpgov': 'corp gov',\n",
    "                  'govtribe': 'provides real-time federal contracting market intel',\n",
    "                  'dragonglass': 'dragon glass', 'gorakpur': 'Gorakhpur', 'MangoPay': 'Mango Pay',\n",
    "                  'chigoe': 'sub-tropical climates', 'BingoBox': 'an investment company', 'èµ°go': 'go',\n",
    "                  'followingorder': 'following order', 'pangolinminer': 'pangolin miner',\n",
    "                  'negosiation': 'negotiation', 'lexigographers': 'lexicographers', 'algorithom': 'algorithm',\n",
    "                  'unforgottable': 'unforgettable', 'wellsfargoemail': 'wellsfargo email',\n",
    "                  'daigonal': 'diagonal', 'Pangoro': 'cantankerous Pokemon', 'negotiotions': 'negotiations',\n",
    "                  'Swissgolden': 'Swiss golden', 'google4': 'google', 'Agoraki': 'Ago raki',\n",
    "                  'Garthago': 'Carthago', 'Stegosauri': 'stegosaurus', 'ergophobia': 'ergo phobia',\n",
    "                  'bigolive': 'big olive', 'bittergoat': 'bitter goat', 'naggots': 'faggots',\n",
    "                  'googology': 'online encyclopedia', 'algortihms': 'algorithms', 'bengolis': 'Bengalis',\n",
    "                  'fingols': 'Finnish people are supposedly descended from Mongols',\n",
    "                  'savethechildren': 'save thechildren',\n",
    "                  'stopings': 'stoping', 'stopsits': 'stop sits', 'stopsigns': 'stop signs',\n",
    "                  'Galastop': 'Galas top', 'pokestops': 'pokes tops', 'forcestop': 'forces top',\n",
    "                  'Hopstop': 'Hops top', 'stoppingexercises': 'stopping exercises', 'coinstop': 'coins top',\n",
    "                  'stoppef': 'stopped', 'workaway': 'work away', 'snazzyway': 'snazzy way',\n",
    "                  'Rewardingways': 'Rewarding ways', 'cloudways': 'cloud ways', 'Cloudways': 'Cloud ways',\n",
    "                  'Brainsway': 'Brains way', 'nesraway': 'nearaway',\n",
    "                  'AlwaysHired': 'Always Hired', 'expessway': 'expressway', 'Syncway': 'Sync way',\n",
    "                  'LeewayHertz': 'Blockchain Company', 'towayrds': 'towards', 'swayable': 'sway able',\n",
    "                  'Telloway': 'Tello way', 'palsmodium': 'plasmodium', 'Gobackmodi': 'Goback modi',\n",
    "                  'comodies': 'corodies', 'islamphobic': 'islam phobic', 'islamphobia': 'islam phobia',\n",
    "                  'citiesbetter': 'cities better', 'betterv3': 'better', 'betterDtu': 'better Dtu',\n",
    "                  'Babadook': 'a horror drama film', 'Ahemadabad': 'Ahmadabad', 'faidabad': 'Faizabad',\n",
    "                  'Amedabad': 'Ahmedabad', 'kabadii': 'kabaddi', 'badmothing': 'badmouthing',\n",
    "                  'badminaton': 'badminton', 'badtameezdil': 'badtameez dil', 'badeffects': 'bad effects',\n",
    "                  'âˆ bad': 'bad', 'ahemadabad': 'Ahmadabad', 'embaded': 'embased', 'Isdhanbad': 'Is dhanbad',\n",
    "                  'badgermoles': 'enormous, blind mammal', 'allhabad': 'Allahabad', 'ghazibad': 'ghazi bad',\n",
    "                  'htderabad': 'Hyderabad', 'Auragabad': 'Aurangabad', 'ahmedbad': 'Ahmedabad',\n",
    "                  'ahmdabad': 'Ahmadabad', 'alahabad': 'Allahabad',\n",
    "                  'Hydeabad': 'Hyderabad', 'Gyroglove': 'wearable technology', 'foodlovee': 'food lovee',\n",
    "                  'slovenised': 'slovenia', 'handgloves': 'hand gloves', 'lovestep': 'love step',\n",
    "                  'lovejihad': 'love jihad', 'RolloverBox': 'Rollover Box', 'stupidedt': 'stupidest',\n",
    "                  'toostupid': 'too stupid',\n",
    "                  'pakistanisbeautiful': 'pakistanis beautiful', 'ispakistan': 'is pakistan',\n",
    "                  'inpersonations': 'impersonations', 'medicalperson': 'medical person',\n",
    "                  'interpersonation': 'inter personation', 'workperson': 'work person',\n",
    "                  'personlich': 'person lich', 'persoenlich': 'person lich',\n",
    "                  'middleperson': 'middle person', 'personslized': 'personalized',\n",
    "                  'personifaction': 'personification', 'welcomemarriage': 'welcome marriage',\n",
    "                  'come2': 'come to', 'upcomedians': 'up comedians', 'overvcome': 'overcome',\n",
    "                  'talecome': 'tale come', 'cometitive': 'competitive', 'arencome': 'aren come',\n",
    "                  'achecomes': 'ache comes', 'ã€come': 'come',\n",
    "                  'comepleted': 'completed', 'overcomeanxieties': 'overcome anxieties',\n",
    "                  'demigirl': 'demi girl', 'gridgirl': 'female models of the race', 'halfgirlfriend': 'half girlfriend',\n",
    "                  'girlriend': 'girlfriend', 'fitgirl': 'fit girl', 'girlfrnd': 'girlfriend', 'awrong': 'aw rong',\n",
    "                  'northcap': 'north cap', 'productionsupport': 'production support',\n",
    "                  'Designbold': 'Online Photo Editor Design Studio',\n",
    "                  'skyhold': 'sky hold', 'shuoldnt': 'shouldnt', 'anarold': 'Android', 'yaerold': 'year old',\n",
    "                  'soldiders': 'soldiers', 'indrold': 'Android', 'blindfoldedly': 'blindfolded',\n",
    "                  'overcold': 'over cold', 'Goldmont': 'microarchitecture in Intel', 'boldspot': 'bolds pot',\n",
    "                  'Rankholders': 'Rank holders', 'cooldrink': 'cool drink', 'beltholders': 'belt holders',\n",
    "                  'GoldenDict': 'open-source dictionary program', 'softskill': 'softs kill',\n",
    "                  'Cooldige': 'the 30th president of the United States',\n",
    "                  'newkiller': 'new killer', 'skillselect': 'skills elect', 'nonskilled': 'non skilled',\n",
    "                  'killyou': 'kill you', 'Skillport': 'Army e-Learning Program', 'unkilled': 'un killed',\n",
    "                  'killikng': 'killing', 'killograms': 'kilograms',\n",
    "                  'Worldkillers': 'World killers', 'reskilled': 'skilled',\n",
    "                  'killedshivaji': 'killed shivaji', 'honorkillings': 'honor killings',\n",
    "                  'skillclasses': 'skill classes', 'microskills': 'micros kills',\n",
    "                  'Skillselect': 'Skills elect', 'ratkill': 'rat kill',\n",
    "                  'pleasegive': 'please give', 'flashgive': 'flash give',\n",
    "                  'southerntelescope': 'southern telescope', 'westsouth': 'west south',\n",
    "                  'southAfricans': 'south Africans', 'Joboutlooks': 'Job outlooks', 'joboutlook': 'job outlook',\n",
    "                  'Outlook365': 'Outlook 365', 'Neulife': 'Neu life', 'qualifeid': 'qualified',\n",
    "                  'nullifed': 'nullified', 'lifeaffect': 'life affect', 'lifestly': 'lifestyle',\n",
    "                  'aristocracylifestyle': 'aristocracy lifestyle', 'antilife': 'anti life',\n",
    "                  'afterafterlife': 'after afterlife', 'lifestylye': 'lifestyle', 'prelife': 'pre life',\n",
    "                  'lifeute': 'life ute', 'liferature': 'literature',\n",
    "                  'securedlife': 'secured life', 'doublelife': 'double life', 'antireligion': 'anti religion',\n",
    "                  'coreligionist': 'co religionist', 'petrostates': 'petro states', 'otherstates': 'others tates',\n",
    "                  'spacewithout': 'space without', 'withoutyou': 'without you',\n",
    "                  'withoutregistered': 'without registered', 'weightwithout': 'weight without',\n",
    "                  'withoutcheck': 'without check', 'milkwithout': 'milk without',\n",
    "                  'Highschoold': 'High school', 'memoney': 'money', 'moneyof': 'mony of', 'Oneplus': 'OnePlus',\n",
    "                  'OnePlus': 'Chinese smartphone manufacturer', 'Beerus': 'the God of Destruction',\n",
    "                  'takeoverr': 'takeover', 'demonetizedd': 'demonetized', 'polyhouse': 'Polytunnel',\n",
    "                  'Elitmus': 'eLitmus', 'eLitmus': 'Indian company that helps companies in hiring employees',\n",
    "                  'becone': 'become', 'nestaway': 'nest away', 'takeoverrs': 'takeovers', 'Istop': 'I stop',\n",
    "                  'Austira': 'Australia', 'germeny': 'Germany', 'mansoon': 'man soon',\n",
    "                  'worldmax': 'wholesaler of drum parts',\n",
    "                  'ammusement': 'amusement', 'manyare': 'many are', 'supplymentary': 'supply mentary',\n",
    "                  'timesup': 'times up', 'homologus': 'homologous', 'uimovement': 'ui movement', 'spause': 'spouse',\n",
    "                  'aesexual': 'asexual', 'Iovercome': 'I overcome', 'developmeny': 'development',\n",
    "                  'hindusm': 'hinduism', 'sexpat': 'sex tourism', 'sunstop': 'sun stop', 'polyhouses': 'Polytunnel',\n",
    "                  'usefl': 'useful', 'Fundamantal': 'fundamental', 'environmentai': 'environmental',\n",
    "                  'Redmi': 'Xiaomi Mobile', 'Loy Machedo': ' Motivational Speaker ', 'unacademy': 'Unacademy',\n",
    "                  'Boruto': 'Naruto Next Generations', 'Upwork': 'Up work',\n",
    "                  'Unacademy': 'educational technology company',\n",
    "                  'HackerRank': 'Hacker Rank', 'upwork': 'up work', 'Chromecast': 'Chrome cast',\n",
    "                  'microservices': 'micro services', 'Undertale': 'video game', 'undergraduation': 'under graduation',\n",
    "                  'chapterwise': 'chapter wise', 'twinflame': 'twin flame', 'Hotstar': 'Hot star',\n",
    "                  'blockchains': 'blockchain',\n",
    "                  'darkweb': 'dark web', 'Microservices': 'Micro services', 'Nearbuy': 'Nearby',\n",
    "                  ' Padmaavat ': ' Padmavati ', ' padmavat ': ' Padmavati ', ' Padmaavati ': ' Padmavati ',\n",
    "                  ' Padmavat ': ' Padmavati ', ' internshala ': ' internship and online training platform in India ',\n",
    "                  'dream11': ' fantasy sports platform in India ', 'conciousnesss': 'consciousnesses',\n",
    "                  'Dream11': ' fantasy sports platform in India ', 'cointry': 'country', ' coinvest ': ' invest ',\n",
    "                  '23 andme': 'privately held personal genomics and biotechnology company in California',\n",
    "                  'Trumpism': 'philosophy and politics espoused by Donald Trump',\n",
    "                  'Trumpian': 'viewpoints of President Donald Trump', 'Trumpists': 'admirer of Donald Trump',\n",
    "                  'coincidents': 'coincidence', 'coinsized': 'coin sized', 'coincedences': 'coincidences',\n",
    "                  'cointries': 'countries', 'coinsidered': 'considered', 'coinfirm': 'confirm',\n",
    "                  'humilates':'humiliates', 'vicevice':'vice vice', 'politicak':'political', 'Sumaterans':'Sumatrans',\n",
    "                  'Kamikazis':'Kamikazes', 'unmoraled':'unmoral', 'eduacated':'educated', 'moraled':'morale',\n",
    "                  'Amharc':'Amarc', 'where Burkhas':'wear Burqas', 'Baloochistan':'Balochistan', 'durgahs':'durgans',\n",
    "                  'illigitmate':'illegitimate', 'hillum':'helium','treatens':'threatens','mutiliating':'mutilating',\n",
    "                  'speakingly':'speaking', 'pretex':'pretext', 'menstruateion':'menstruation', \n",
    "                  'genocidizing':'genociding', 'maratis':'Maratism','Parkistinian':'Pakistani', 'SPEICIAL':'SPECIAL',\n",
    "                  'REFERNECE':'REFERENCE', 'provocates':'provokes', 'FAMINAZIS':'FEMINAZIS', 'repugicans':'republicans',\n",
    "                  'tonogenesis':'tone', 'winor':'win', 'redicules':'ridiculous', 'Beluchistan':'Balochistan', \n",
    "                  'volime':'volume', 'namaj':'namaz', 'CONgressi':'Congress', 'Ashifa':'Asifa', 'queffing':'queefing',\n",
    "                  'montheistic':'nontheistic', 'Rajsthan':'Rajasthan', 'Rajsthanis':'Rajasthanis', 'specrum':'spectrum',\n",
    "                  'brophytes':'bryophytes', 'adhaar':'Adhara', 'slogun':'slogan', 'harassd':'harassed',\n",
    "                  'transness':'trans gender', 'Insdians':'Indians', 'Trampaphobia':'Trump aphobia', 'attrected':'attracted',\n",
    "                  'Yahtzees':'Yahtzee', 'thiests':'atheists', 'thrir':'their', 'extraterestrial':'extraterrestrial',\n",
    "                  'silghtest':'slightest', 'primarty':'primary','brlieve':'believe', 'fondels':'fondles',\n",
    "                  'loundly':'loudly', 'bootythongs':'booty thongs', 'understamding':'understanding', 'degenarate':'degenerate',\n",
    "                  'narsistic':'narcistic', 'innerskin':'inner skin','spectulated':'speculated', 'hippocratical':'Hippocratical',\n",
    "                  'itstead':'instead', 'parralels':'parallels', 'sloppers':'slippers'\n",
    "                  }\n",
    "\n",
    "\n",
    "num_partitions = 20 # number of partitions to split dataframe\n",
    "num_cores = psutil.cpu_count()  # number of cores on your machine\n",
    "from functools import  partial\n",
    "def df_parallelize_run(df, func):\n",
    "    \n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    #df = sp.vstack(pool.map(func, df_split), format='csr') faster and mem efficient for\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_diacritics(s):\n",
    "    return ''.join(c for c in normalize('NFKD', s.replace('Ã¸', 'o').replace('Ã˜', 'O').replace('â»', '-').replace('â‚‹', '-'))\n",
    "                  if category(c) != 'Mn')\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # remove_diacritics donÂ´t' ->  'don t'\n",
    "    #text = remove_diacritics(text)\n",
    "    return text\n",
    "\n",
    "# clean numbers\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    text = re.sub(r'(\\d+)(e)(\\d+)','\\g<1> \\g<3>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pre_clean_rare_words(text):\n",
    "    for rare_word in rare_words_mapping:\n",
    "        if rare_word in text:\n",
    "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_misspell(text):\n",
    "    for bad_word in mispell_dict:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, mispell_dict[bad_word])\n",
    "    return text\n",
    "\n",
    "regular_punct = list(string.punctuation)\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "# do not spacing - and .\n",
    "all_punct.remove('-')\n",
    "all_punct.remove('.')\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_bad_case_words(text):\n",
    "    for bad_word in bad_case_words:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
    "    return text\n",
    "\n",
    "mis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\n",
    "mis_connect_re  = re.compile('(%s)' % '|'.join(mis_connect_list))\n",
    "\n",
    "mis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n",
    "                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n",
    "                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n",
    "                      'Whybis':'Why is', 'laowhy86':'Foreigners who do not respect China',\n",
    "                      'Whyco-education':'Why co-education',\n",
    "                      \"Howddo\":\"How do\", 'Howeber':'However', 'Showh':'Show',\n",
    "                      \"Willowmagic\":'Willow magic', 'WillsEye':'Will Eye', 'Williby':'will by',\n",
    "                     'pretextt':'pre text','aÉ´á´…':'and','amette':'annette','aá´›':'at','Tridentinus':'mushroom',\n",
    "                    'dailycaller':'daily caller', \"â„¢\":'trade mark'}\n",
    "\n",
    "def spacing_some_connect_words(text):\n",
    "    \"\"\"\n",
    "    'Whyare' -> 'Why are'\n",
    "    \"\"\"\n",
    "    ori = text\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])\n",
    "\n",
    "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
    "    text = text.replace(\"What sApp\", ' WhatsApp ')\n",
    "    text = remove_space(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# clean repeated letters\n",
    "def clean_repeat_words(text):\n",
    "    \n",
    "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text) #this one is causing few issues(fixed via monkey patching in other dicts for now), need to check it..\n",
    "#     text = re.sub(r\"(-+|\\.+)\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    preprocess text main steps\n",
    "    \"\"\"\n",
    "\n",
    "    text = pre_clean_rare_words(text)\n",
    "    text = clean_misspell(text)\n",
    "    text = spacing_some_connect_words(text)\n",
    "    text = clean_bad_case_words(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_clean_wrapper(df):\n",
    "    df = df.apply(preprocess)\n",
    "    return df\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    x = str(x).replace(' s ','').replace('â€¦', ' ').replace('â€”','-').replace('â€¢Â°â€¢Â°â€¢','') #should be broken down to regexs (lazy to do it haha)\n",
    "    for punct in \"/-'\":\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "\n",
    "    x = re.sub(r'(by|been|and|are|for|it|TV|already|justhow|some|had|is|will|would|should|shall|must|can|his|here|there|them|these|their|has|have|the|be|that|not|was|he|just|they|who)(how)', '\\g<1> \\g<2>', x) \n",
    "\n",
    "    return x\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip() \n",
    "    return s\n",
    "\n",
    "def bert_preprocess(df,col):\n",
    "    df[col] = df[col].apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "    df[col] = df_parallelize_run(df[col], text_clean_wrapper)\n",
    "    df[col] = df[col].apply(lambda x: clean_text(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_len = len(df_train)\n",
    "data = pd.concat([df_train,df_test])\n",
    "\n",
    "\n",
    "data = bert_preprocess(data,'question_title')\n",
    "data = bert_preprocess(data,'question_body')\n",
    "data = bert_preprocess(data,'answer')\n",
    "\n",
    "\n",
    "\n",
    "df_train = data[:train_len]\n",
    "df_test = data[train_len:]\n",
    "df_train.loc[1,'question_type_spelling'] = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:03, 154.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:530: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:531: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:532: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:533: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:03, 153.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    }
   ],
   "source": [
    "#bert  base æ¨¡åž‹\n",
    "\n",
    "df_train['is_aug'] =0\n",
    "\n",
    "if OFFLINE == False:\n",
    "    PATH = '/kaggle/input/google-quest-challenge/'\n",
    "    BERT_PATH = '/kaggle/input/tf-bert-base-uncased/bert_base/'\n",
    "    vocab_file =BERT_PATH+\"bert-base-uncased-vocab.txt\"\n",
    "\n",
    "    tokenizer = BertTokenizer(vocab_file)\n",
    "\n",
    "\n",
    "    \n",
    "else:\n",
    "    PATH = '../input/'\n",
    "\n",
    "    BERT_PATH = '/home/wk/Bert_Pretrained/bert_base/'\n",
    "    vocab_file =BERT_PATH+\"bert-base-uncased-vocab.txt\"\n",
    "\n",
    "    tokenizer = BertTokenizer(vocab_file)\n",
    "    \n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "\n",
    "    \n",
    "def prepare_data_q(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens)>max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        first_sep = True\n",
    "        current_segment_id = 0\n",
    "        for token in tokens:\n",
    "            segments.append(current_segment_id)\n",
    "            if token == \"[SEP]\":\n",
    "                if first_sep:\n",
    "                    first_sep = False \n",
    "                else:\n",
    "                    current_segment_id = 1\n",
    "                    \n",
    "        return segments+[1]*(max_seq_length-len(segments))\n",
    "\n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_q(title, question,  max_sequence_length,\n",
    "                    t_max_len=35, q_max_len=73):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(question)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 3) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 3 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        question_body_ = str(instance['question_body']).lower()\n",
    "\n",
    "        question_title_, question_body_  = _trim_input_q(question_title_, question_body_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=469)\n",
    "\n",
    "        stoken = [\"[CLS]\"] + question_title_ + [\"[SEP]\"]+ question_body_ + [\"[SEP]\"]\n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    Y=[]\n",
    "    for i in q_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2   \n",
    "\n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in q_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "            data.loc[data.is_aug==1,i+\"_wt\"] = 0.6\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "\n",
    "def create_model_q(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "    config = BertConfig().from_pretrained(BERT_PATH+'bert-base-uncased-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    bert = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in bert.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = bert(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    \n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(q_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "\n",
    "y_test = np.zeros((len(test_inputs), len(targets)))\n",
    "all_predictions = []\n",
    "score = []\n",
    "score2 = []\n",
    "score3 = []\n",
    "start_time = time.time()\n",
    "if OFFLINE == False:\n",
    "    model_path =  '/kaggle/input/dy-qa-bertbase-v5/quest_bert_weights/quest_bert_weights'\n",
    "else:\n",
    "    model_path = 'quest_bert_weights'\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "\n",
    "bert_base_oof_train = df_train[['qa_id']+q_col].copy()\n",
    "bert_base_oof_train[q_col] = 0\n",
    "\n",
    "bert_base_oof_test = df_test[['qa_id']].copy()\n",
    "for i in q_col:\n",
    "    bert_base_oof_test[i]=0\n",
    "\n",
    "    \n",
    "def compute_output_arrays2(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE or TEST:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,q_col)\n",
    "        \n",
    "    model_weight = f'{model_path}/robert_fold{fold_}.h5'\n",
    "    model = create_model_q(False)\n",
    "    model.load_weights(model_weight)\n",
    "    if OFFLINE or TEST:\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=30)).squeeze().T\n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        bert_base_oof_train.loc[val_idx, q_col]= valid_pred\n",
    "    \n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=30)).squeeze().T\n",
    "    \n",
    "    bert_base_oof_test.loc[:,q_col] += test_pred/5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_a(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens)>max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        first_sep = True\n",
    "        current_segment_id = 0\n",
    "        for token in tokens:\n",
    "            segments.append(current_segment_id)\n",
    "            if token == \"[SEP]\":\n",
    "                if first_sep:\n",
    "                    first_sep = False \n",
    "                else:\n",
    "                    current_segment_id = 1\n",
    "                    \n",
    "        return segments+[1]*(max_seq_length-len(segments))\n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_a(title, answer,  max_sequence_length,\n",
    "                    t_max_len=40, q_max_len=468):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(answer)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 3) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 3 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        answer_ = str(instance['answer']).lower()\n",
    "\n",
    "        question_title_, answer_ = _trim_input_a(question_title_, answer_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=469)\n",
    "\n",
    "        stoken = [\"[CLS]\"] + question_title_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"] \n",
    "\n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])\n",
    "    Y=[]\n",
    "    for i in a_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2 \n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in a_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "           \n",
    "            \n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model_a(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "\n",
    "    config = BertConfig().from_pretrained(BERT_PATH+'bert-base-uncased-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    bert = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in bert.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = bert(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(a_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "for i in a_col:\n",
    "    bert_base_oof_test[i]=0\n",
    "    bert_base_oof_train[i]=0\n",
    "    \n",
    "if OFFLINE==False:\n",
    "\n",
    "    model_path = '/kaggle/input/dy-qa-bertbase-v5/answer_bert_weights/answer_bert_weights'\n",
    "    \n",
    "else:\n",
    "    model_path = 'answer_bert_weights'\n",
    "   \n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) \n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,a_col)\n",
    "\n",
    "    model_weight = f'{model_path}/bert_fold{fold_}.h5'\n",
    "    model = create_model_a(False)\n",
    "\n",
    "    model.load_weights(model_weight)\n",
    "    \n",
    "    if OFFLINE:\n",
    "\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=30)).squeeze().T  \n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        bert_base_oof_train.loc[val_idx, a_col]= valid_pred\n",
    "\n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=30)).squeeze().T\n",
    "    bert_base_oof_test.loc[:,a_col] += test_pred/5\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.961191</td>\n",
       "      <td>0.725247</td>\n",
       "      <td>0.173344</td>\n",
       "      <td>0.528154</td>\n",
       "      <td>0.700813</td>\n",
       "      <td>0.487942</td>\n",
       "      <td>0.721043</td>\n",
       "      <td>0.703937</td>\n",
       "      <td>0.551804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953051</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.587522</td>\n",
       "      <td>0.954462</td>\n",
       "      <td>0.957584</td>\n",
       "      <td>0.811732</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.024657</td>\n",
       "      <td>0.782065</td>\n",
       "      <td>0.915969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.880360</td>\n",
       "      <td>0.522571</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.773088</td>\n",
       "      <td>0.812108</td>\n",
       "      <td>0.927158</td>\n",
       "      <td>0.561994</td>\n",
       "      <td>0.475994</td>\n",
       "      <td>0.273847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700030</td>\n",
       "      <td>0.958450</td>\n",
       "      <td>0.664879</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.987043</td>\n",
       "      <td>0.897054</td>\n",
       "      <td>0.918534</td>\n",
       "      <td>0.118375</td>\n",
       "      <td>0.059186</td>\n",
       "      <td>0.912805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.696303</td>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.835512</td>\n",
       "      <td>0.926853</td>\n",
       "      <td>0.944919</td>\n",
       "      <td>0.615468</td>\n",
       "      <td>0.507573</td>\n",
       "      <td>0.107860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918134</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.597710</td>\n",
       "      <td>0.976113</td>\n",
       "      <td>0.977637</td>\n",
       "      <td>0.869402</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.019341</td>\n",
       "      <td>0.923468</td>\n",
       "      <td>0.919766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.904683</td>\n",
       "      <td>0.496734</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.760534</td>\n",
       "      <td>0.746104</td>\n",
       "      <td>0.921160</td>\n",
       "      <td>0.558893</td>\n",
       "      <td>0.419067</td>\n",
       "      <td>0.047865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743222</td>\n",
       "      <td>0.964888</td>\n",
       "      <td>0.682767</td>\n",
       "      <td>0.978288</td>\n",
       "      <td>0.989304</td>\n",
       "      <td>0.892740</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>0.164054</td>\n",
       "      <td>0.761306</td>\n",
       "      <td>0.930121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.936519</td>\n",
       "      <td>0.500997</td>\n",
       "      <td>0.015563</td>\n",
       "      <td>0.854394</td>\n",
       "      <td>0.850593</td>\n",
       "      <td>0.907602</td>\n",
       "      <td>0.677483</td>\n",
       "      <td>0.604690</td>\n",
       "      <td>0.149349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748084</td>\n",
       "      <td>0.922215</td>\n",
       "      <td>0.671516</td>\n",
       "      <td>0.967731</td>\n",
       "      <td>0.967122</td>\n",
       "      <td>0.858240</td>\n",
       "      <td>0.346186</td>\n",
       "      <td>0.144836</td>\n",
       "      <td>0.406702</td>\n",
       "      <td>0.910667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.961191                0.725247   \n",
       "1     46                             0.880360                0.522571   \n",
       "2     70                             0.944046                0.696303   \n",
       "3    132                             0.904683                0.496734   \n",
       "4    200                             0.936519                0.500997   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.173344                      0.528154   \n",
       "1                 0.003797                      0.773088   \n",
       "2                 0.011950                      0.835512   \n",
       "3                 0.003543                      0.760534   \n",
       "4                 0.015563                      0.854394   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.700813                               0.487942   \n",
       "1               0.812108                               0.927158   \n",
       "2               0.926853                               0.944919   \n",
       "3               0.746104                               0.921160   \n",
       "4               0.850593                               0.907602   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.721043                       0.703937   \n",
       "1                         0.561994                       0.475994   \n",
       "2                         0.615468                       0.507573   \n",
       "3                         0.558893                       0.419067   \n",
       "4                         0.677483                       0.604690   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.551804  ...               0.953051        0.905556   \n",
       "1               0.273847  ...               0.700030        0.958450   \n",
       "2               0.107860  ...               0.918134        0.943299   \n",
       "3               0.047865  ...               0.743222        0.964888   \n",
       "4               0.149349  ...               0.748084        0.922215   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.587522          0.954462          0.957584   \n",
       "1                     0.664879          0.974244          0.987043   \n",
       "2                     0.597710          0.976113          0.977637   \n",
       "3                     0.682767          0.978288          0.989304   \n",
       "4                     0.671516          0.967731          0.967122   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.811732                  0.021013               0.024657   \n",
       "1             0.897054                  0.918534               0.118375   \n",
       "2             0.869402                  0.028645               0.019341   \n",
       "3             0.892740                  0.921466               0.164054   \n",
       "4             0.858240                  0.346186               0.144836   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.782065             0.915969  \n",
       "1                        0.059186             0.912805  \n",
       "2                        0.923468             0.919766  \n",
       "3                        0.761306             0.930121  \n",
       "4                        0.406702             0.910667  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_base_oof_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OFFLINE:\n",
    "\n",
    "    def compute_spearmanr_ignore_nan(trues, preds):\n",
    "        rhos = []\n",
    "        for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "            rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "        return np.nanmean(rhos)\n",
    "\n",
    "\n",
    "    print(compute_spearmanr_ignore_nan(bert_base_oof_train[targets].values,df_train[targets].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:01, 296.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:545: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:546: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:547: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:548: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:01, 383.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    }
   ],
   "source": [
    "#roberta base 456 æ¨¡åž‹\n",
    "df_train['is_aug'] =0\n",
    "\n",
    "if OFFLINE == False:\n",
    "    PATH = '/kaggle/input/google-quest-challenge/'\n",
    "    ROBERTA_PATH = '/kaggle/input/robert-base-tf2/robert_base/'\n",
    "    vocab_file =ROBERTA_PATH+\"roberta-base-vocab.json\"\n",
    "    merges_file =ROBERTA_PATH+\"roberta-base-merges.txt\"\n",
    "    tokenizer = RobertaTokenizer(vocab_file,merges_file)\n",
    "    \n",
    "else:\n",
    "    PATH = '../input/'\n",
    "\n",
    "    ROBERTA_PATH = '/home/wk/Bert_Pretrained/robert_base/'\n",
    "    vocab_file =ROBERTA_PATH+\"roberta-base-vocab.json\"\n",
    "    merges_file =ROBERTA_PATH+\"roberta-base-merges.txt\"\n",
    "\n",
    "    tokenizer = RobertaTokenizer(vocab_file,merges_file)\n",
    "    \n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_data_q(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    robertaè¾“å…¥ segments idåªèƒ½æ˜¯0\n",
    "\n",
    "    '''\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = [0]*max_seq_length\n",
    "    #     current_segment_id = 0\n",
    "    #     for token in tokens:\n",
    "    #         if token == \"</s>\":\n",
    "    #             segments.extend([1] * (max_seq_length - len(segments)))\n",
    "    #             return segments \n",
    "    #         else:\n",
    "    #             segments.append(current_segment_id)\n",
    "\n",
    "        return segments \n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_q(title, question,  max_sequence_length,\n",
    "                    t_max_len=35, q_max_len=73):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(question)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 4) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 4 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        question_body_ = str(instance['question_body']).lower()\n",
    "\n",
    "        question_title_, question_body_  = _trim_input_q(question_title_, question_body_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=468)\n",
    "\n",
    "        stoken = [\"<s>\"] + question_title_ + [\"</s>\"] +[\"</s>\"]+ question_body_ + [\"</s>\"] \n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])\n",
    "    Y=[]\n",
    "    for i in q_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2   \n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2   \n",
    "\n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in q_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "            data.loc[data.is_aug==1,i+\"_wt\"] = 0.6\n",
    "            \n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model_q(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "\n",
    "    config = RobertaConfig().from_pretrained(ROBERTA_PATH+'roberta-base-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    roberta = TFRobertaModel.from_pretrained(ROBERTA_PATH+'roberta-base-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in roberta.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = roberta(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    \n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(q_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "\n",
    "y_test = np.zeros((len(test_inputs), len(targets)))\n",
    "all_predictions = []\n",
    "score = []\n",
    "score2 = []\n",
    "score3 = []\n",
    "start_time = time.time()\n",
    "if OFFLINE == False:\n",
    "    model_path = '/kaggle/input/dy-qa-robertbase-v3/quest_robert_weights_v2/quest_robert_weights_v2/'\n",
    "else:\n",
    "    model_path = 'quest_robert_weights_v2'  #v3\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "\n",
    "roberta_base_oof_train456 = df_train[['qa_id']+q_col].copy()\n",
    "roberta_base_oof_train456[q_col] = 0\n",
    "\n",
    "roberta_base_oof_test456 = df_test[['qa_id']].copy()\n",
    "for i in q_col:\n",
    "    roberta_base_oof_test456[i]=0\n",
    "\n",
    "    \n",
    "def compute_output_arrays2(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE or TEST:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,q_col)\n",
    "        \n",
    "    model_weight = f'{model_path}/robert_fold{fold_}.h5'\n",
    "    model = create_model_q(False)\n",
    "    model.load_weights(model_weight)\n",
    "    if OFFLINE or TEST:\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=30)).squeeze().T\n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        roberta_base_oof_train456.loc[val_idx, q_col]= valid_pred\n",
    "    \n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=30)).squeeze().T\n",
    "    \n",
    "    roberta_base_oof_test456.loc[:,q_col] += test_pred/5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_a(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    robertaè¾“å…¥ segments idåªèƒ½æ˜¯0\n",
    "\n",
    "    '''\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = [0]*max_seq_length\n",
    "    #     current_segment_id = 0\n",
    "    #     for token in tokens:\n",
    "    #         if token == \"</s>\":\n",
    "    #             segments.extend([1] * (max_seq_length - len(segments)))\n",
    "    #             return segments \n",
    "    #         else:\n",
    "    #             segments.append(current_segment_id)\n",
    "\n",
    "        return segments \n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_a(title, answer,  max_sequence_length,\n",
    "                    t_max_len=40, q_max_len=468):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(answer)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 4) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 4 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        answer_ = str(instance['answer']).lower()\n",
    "\n",
    "        question_title_, answer_ = _trim_input_a(question_title_, answer_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=468)\n",
    "\n",
    "        stoken = [\"<s>\"] + question_title_ + [\"</s>\"] +[\"</s>\"]+ answer_ + [\"</s>\"] \n",
    "\n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])\n",
    "    Y=[]\n",
    "    for i in a_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2 \n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in a_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "            data.loc[data.is_aug==1,i+\"_wt\"] = 0.6\n",
    "            \n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "\n",
    "def create_model_a(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "\n",
    "    config = RobertaConfig().from_pretrained(ROBERTA_PATH+'roberta-base-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    roberta = TFRobertaModel.from_pretrained(ROBERTA_PATH+'roberta-base-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in roberta.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = roberta(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(a_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "for i in a_col:\n",
    "    roberta_base_oof_test456[i]=0\n",
    "    roberta_base_oof_train456[i]=0\n",
    "    \n",
    "if OFFLINE==False:\n",
    "\n",
    "    model_path = '/kaggle/input/dy-qa-robertbase-v3/answer_robert_weights_v2/answer_robert_weights_v2/'\n",
    "    \n",
    "else:\n",
    "    model_path = 'answer_robert_weights_v2'\n",
    "   \n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) \n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,a_col)\n",
    "\n",
    "    model_weight = f'{model_path}/robert_fold{fold_}.h5'\n",
    "    model = create_model_a(False)\n",
    "\n",
    "    model.load_weights(model_weight)\n",
    "    \n",
    "    if OFFLINE:\n",
    "\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=30)).squeeze().T  \n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        roberta_base_oof_train456.loc[val_idx, a_col]= valid_pred\n",
    "\n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=30)).squeeze().T\n",
    "    roberta_base_oof_test456.loc[:,a_col] += test_pred/5\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.950777</td>\n",
       "      <td>0.726448</td>\n",
       "      <td>0.147639</td>\n",
       "      <td>0.615816</td>\n",
       "      <td>0.756192</td>\n",
       "      <td>0.613971</td>\n",
       "      <td>0.694905</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.480646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.952165</td>\n",
       "      <td>0.944458</td>\n",
       "      <td>0.609118</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>0.979117</td>\n",
       "      <td>0.883198</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.829164</td>\n",
       "      <td>0.932919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.875885</td>\n",
       "      <td>0.489989</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.793948</td>\n",
       "      <td>0.796226</td>\n",
       "      <td>0.938474</td>\n",
       "      <td>0.563718</td>\n",
       "      <td>0.461578</td>\n",
       "      <td>0.221516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646666</td>\n",
       "      <td>0.956594</td>\n",
       "      <td>0.663020</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.986059</td>\n",
       "      <td>0.894175</td>\n",
       "      <td>0.926533</td>\n",
       "      <td>0.116449</td>\n",
       "      <td>0.058545</td>\n",
       "      <td>0.924193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.939903</td>\n",
       "      <td>0.729173</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.837289</td>\n",
       "      <td>0.948590</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.604034</td>\n",
       "      <td>0.506080</td>\n",
       "      <td>0.124191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920988</td>\n",
       "      <td>0.946385</td>\n",
       "      <td>0.611643</td>\n",
       "      <td>0.976766</td>\n",
       "      <td>0.976577</td>\n",
       "      <td>0.870008</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>0.032153</td>\n",
       "      <td>0.879558</td>\n",
       "      <td>0.926882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.752991</td>\n",
       "      <td>0.751811</td>\n",
       "      <td>0.914201</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.427333</td>\n",
       "      <td>0.046669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774303</td>\n",
       "      <td>0.962821</td>\n",
       "      <td>0.685639</td>\n",
       "      <td>0.978380</td>\n",
       "      <td>0.988728</td>\n",
       "      <td>0.886668</td>\n",
       "      <td>0.865386</td>\n",
       "      <td>0.155872</td>\n",
       "      <td>0.783616</td>\n",
       "      <td>0.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.934425</td>\n",
       "      <td>0.496362</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.854834</td>\n",
       "      <td>0.874728</td>\n",
       "      <td>0.923934</td>\n",
       "      <td>0.657771</td>\n",
       "      <td>0.621513</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794235</td>\n",
       "      <td>0.916823</td>\n",
       "      <td>0.660571</td>\n",
       "      <td>0.966857</td>\n",
       "      <td>0.964466</td>\n",
       "      <td>0.849980</td>\n",
       "      <td>0.266332</td>\n",
       "      <td>0.133376</td>\n",
       "      <td>0.526610</td>\n",
       "      <td>0.924470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.950777                0.726448   \n",
       "1     46                             0.875885                0.489989   \n",
       "2     70                             0.939903                0.729173   \n",
       "3    132                             0.907176                0.501549   \n",
       "4    200                             0.934425                0.496362   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.147639                      0.615816   \n",
       "1                 0.004140                      0.793948   \n",
       "2                 0.011156                      0.837289   \n",
       "3                 0.004372                      0.752991   \n",
       "4                 0.016459                      0.854834   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.756192                               0.613971   \n",
       "1               0.796226                               0.938474   \n",
       "2               0.948590                               0.957333   \n",
       "3               0.751811                               0.914201   \n",
       "4               0.874728                               0.923934   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.694905                       0.634328   \n",
       "1                         0.563718                       0.461578   \n",
       "2                         0.604034                       0.506080   \n",
       "3                         0.558320                       0.427333   \n",
       "4                         0.657771                       0.621513   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.480646  ...               0.952165        0.944458   \n",
       "1               0.221516  ...               0.646666        0.956594   \n",
       "2               0.124191  ...               0.920988        0.946385   \n",
       "3               0.046669  ...               0.774303        0.962821   \n",
       "4               0.123724  ...               0.794235        0.916823   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.609118          0.973658          0.979117   \n",
       "1                     0.663020          0.974026          0.986059   \n",
       "2                     0.611643          0.976766          0.976577   \n",
       "3                     0.685639          0.978380          0.988728   \n",
       "4                     0.660571          0.966857          0.964466   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.883198                  0.008645               0.014220   \n",
       "1             0.894175                  0.926533               0.116449   \n",
       "2             0.870008                  0.034603               0.032153   \n",
       "3             0.886668                  0.865386               0.155872   \n",
       "4             0.849980                  0.266332               0.133376   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.829164             0.932919  \n",
       "1                        0.058545             0.924193  \n",
       "2                        0.879558             0.926882  \n",
       "3                        0.783616             0.920300  \n",
       "4                        0.526610             0.924470  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base_oof_test456.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OFFLINE:\n",
    "    def compute_spearmanr_ignore_nan(trues, preds):\n",
    "        rhos = []\n",
    "        for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "            rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "        return np.nanmean(rhos)\n",
    "\n",
    "\n",
    "    print(compute_spearmanr_ignore_nan(roberta_base_oof_train456[targets].values,df_train[targets].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:199: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:01, 312.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:536: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:538: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "476it [00:01, 366.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_:  0\n",
      "fold_:  1\n",
      "fold_:  2\n",
      "fold_:  3\n",
      "fold_:  4\n"
     ]
    }
   ],
   "source": [
    "# roberta large lb455æ¨¡åž‹\n",
    "if OFFLINE == False:\n",
    "    PATH = '/kaggle/input/google-quest-challenge/'\n",
    "    ROBERTA_PATH = '/kaggle/input/roberta-large-tf2/roberta_large/'\n",
    "    vocab_file =ROBERTA_PATH+\"roberta-large-vocab.json\"\n",
    "    merges_file =ROBERTA_PATH+\"roberta-large-merges.txt\"\n",
    "    tokenizer = RobertaTokenizer(vocab_file,merges_file)\n",
    "    \n",
    "else:\n",
    "    PATH = '../input/'\n",
    "\n",
    "    ROBERTA_PATH = '/home/wk/Bert_Pretrained/roberta_large/'\n",
    "    vocab_file =ROBERTA_PATH+\"roberta-large-vocab.json\"\n",
    "    merges_file =ROBERTA_PATH+\"roberta-large-merges.txt\"\n",
    "\n",
    "    tokenizer = RobertaTokenizer(vocab_file,merges_file)\n",
    "    \n",
    "    \n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_data_q(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    robertaè¾“å…¥ segments idåªèƒ½æ˜¯0\n",
    "\n",
    "    '''\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = [0]*max_seq_length\n",
    "    #     current_segment_id = 0\n",
    "    #     for token in tokens:\n",
    "    #         if token == \"</s>\":\n",
    "    #             segments.extend([1] * (max_seq_length - len(segments)))\n",
    "    #             return segments \n",
    "    #         else:\n",
    "    #             segments.append(current_segment_id)\n",
    "\n",
    "        return segments \n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_q(title, question,  max_sequence_length,\n",
    "                    t_max_len=35, q_max_len=73):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(question)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 4) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 4 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        question_body_ = str(instance['question_body']).lower()\n",
    "\n",
    "        question_title_, question_body_  = _trim_input_q(question_title_, question_body_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=468)\n",
    "\n",
    "        stoken = [\"<s>\"] + question_title_ + [\"</s>\"] +[\"</s>\"]+ question_body_ + [\"</s>\"] \n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])\n",
    "    Y=[]\n",
    "    for i in q_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2   \n",
    "\n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in q_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "            data.loc[data.is_aug==1,i+\"_wt\"] = 0.6\n",
    "            \n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_q(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model_q(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "\n",
    "    config = RobertaConfig().from_pretrained(ROBERTA_PATH+'roberta-large-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    roberta = TFRobertaModel.from_pretrained(ROBERTA_PATH+'roberta-large-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in roberta.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = roberta(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    \n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(q_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "\n",
    "y_test = np.zeros((len(test_inputs), len(targets)))\n",
    "all_predictions = []\n",
    "score = []\n",
    "score2 = []\n",
    "score3 = []\n",
    "start_time = time.time()\n",
    "if OFFLINE == False:\n",
    "    model_path = '/kaggle/input/dy-qa-robertalarge-v5/quest_robert_large_weights/quest_robert_large_weights'\n",
    "else:\n",
    "    model_path = 'quest_robert_large_weights'\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "\n",
    "roberta_large_oof_train = df_train[['qa_id']+q_col].copy()\n",
    "roberta_large_oof_train[q_col] = 0\n",
    "\n",
    "roberta_large_oof_test = df_test[['qa_id']].copy()\n",
    "for i in q_col:\n",
    "    roberta_large_oof_test[i]=0\n",
    "\n",
    "    \n",
    "def compute_output_arrays2(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE or TEST:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,q_col)\n",
    "        \n",
    "    model_weight = f'{model_path}/robert_fold{fold_}.h5'\n",
    "    model = create_model_q(False)\n",
    "    model.load_weights(model_weight)\n",
    "    if OFFLINE or TEST:\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=50)).squeeze().T\n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        roberta_large_oof_train.loc[val_idx, q_col]= valid_pred\n",
    "    \n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=50)).squeeze().T\n",
    "    \n",
    "    roberta_large_oof_test.loc[:,q_col] += test_pred/5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_a(df, tokenizer):\n",
    "    # roberta <s> A </s></s> B </s>\n",
    "    def _get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    robertaè¾“å…¥ segments idåªèƒ½æ˜¯0\n",
    "\n",
    "    '''\n",
    "\n",
    "    def _get_segments(tokens, max_seq_length):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens) > max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = [0]*max_seq_length\n",
    "    #     current_segment_id = 0\n",
    "    #     for token in tokens:\n",
    "    #         if token == \"</s>\":\n",
    "    #             segments.extend([1] * (max_seq_length - len(segments)))\n",
    "    #             return segments \n",
    "    #         else:\n",
    "    #             segments.append(current_segment_id)\n",
    "\n",
    "        return segments \n",
    "\n",
    "\n",
    "    def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def my_pad(text, max_length, tokenizer):\n",
    "        res = tokenizer.tokenize(text)\n",
    "        if len(res) > max_length:\n",
    "            head_length = int(0.25 * max_length)\n",
    "            tail_length = max_length - head_length\n",
    "            res = res[:head_length] + res[-tail_length:]\n",
    "        return res\n",
    "\n",
    "    def _trim_input_a(title, answer,  max_sequence_length,\n",
    "                    t_max_len=40, q_max_len=468):\n",
    "        t = tokenizer.tokenize(title)\n",
    "        q = tokenizer.tokenize(answer)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "\n",
    "        if (t_len + q_len + 4) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                q_max_len = q_max_len + ceil((t_max_len - t_len))\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "\n",
    "            q_new_len = q_max_len\n",
    "\n",
    "            if t_new_len + q_new_len + 4 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"\n",
    "                                 % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
    "\n",
    "    #         head_t_new_len = int(0.25 * t_new_len)\n",
    "    #         tail_t_new_len = t_new_len - head_t_new_len\n",
    "\n",
    "            head_q_new_len = int(0.5 * q_new_len)\n",
    "            tail_q_new_len = q_new_len - head_q_new_len\n",
    "\n",
    "\n",
    "\n",
    "            t = t[:t_new_len] \n",
    "            q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
    "\n",
    "        return t, q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    count=0\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
    "        question_title_ = str(instance['question_title']).lower()\n",
    "        answer_ = str(instance['answer']).lower()\n",
    "\n",
    "        question_title_, answer_ = _trim_input_a(question_title_, answer_,  MAX_SEQUENCE_LENGTH,\n",
    "                                                               t_max_len=40, q_max_len=468)\n",
    "\n",
    "        stoken = [\"<s>\"] + question_title_ + [\"</s>\"] +[\"</s>\"]+ answer_ + [\"</s>\"] \n",
    "\n",
    "\n",
    "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
    "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        input_ids.append(input_ids_)\n",
    "        input_masks.append(input_masks_)\n",
    "        input_segments.append(input_segments_)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32),\n",
    "            np.asarray(input_masks, dtype=np.int32),\n",
    "            np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])\n",
    "    Y=[]\n",
    "    for i in a_col:\n",
    "        Y.append(df[[i,i+\"_wt\"]].values)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        uniq_X = np.unique(X)\n",
    "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def _map(self, x):\n",
    "        return self.mapper.get(x, 0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return list(map(self._map, X))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "label_encoder = {}\n",
    "lbe = LabelEncoder()\n",
    "lbe.fit(df_train[\"category\"])\n",
    "label_encoder[\"category\"]  = lbe\n",
    "\n",
    "lbe2 = LabelEncoder()\n",
    "lbe2.fit(df_train[\"host\"])\n",
    "label_encoder[\"host\"] = lbe2 \n",
    "\n",
    "\n",
    "def get_input(data,is_train):\n",
    "    \n",
    "    data['t_len'] = data['question_title'].apply(lambda x:len(x.split()))\n",
    "    data['b_len'] = data['question_body'].apply(lambda x:len(x.split()))\n",
    "    data['a_len'] = data['answer'].apply(lambda x:len(x.split()))\n",
    "    data['a/t_rate'] = (data['a_len']+1)/(data['b_len']+1)\n",
    "    \n",
    "    if is_train:\n",
    "        for i in a_col:\n",
    "            data[i+\"_wt\"] = 1\n",
    "            data.loc[data.is_aug==1,i+\"_wt\"] = 0.6\n",
    "            \n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        \n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        outputs = compute_output_arrays(data, q_col)\n",
    "        return inputs,outputs\n",
    "    else:\n",
    "        inputs = prepare_data_a(data, tokenizer)\n",
    "        inputs.append(np.array(data[['t_len','b_len','a_len','a/t_rate']]))\n",
    "        category_feat = np.array(label_encoder[\"category\"].transform(data[\"category\"]))\n",
    "        host_feat = np.array(label_encoder[\"host\"].transform(data[\"host\"]))\n",
    "        inputs.extend([category_feat, host_feat])\n",
    "        return inputs       \n",
    "\n",
    "\n",
    "test_inputs = get_input(df_test,is_train=False)\n",
    "\n",
    "\n",
    "def create_model_a(bert_trainabel=True):\n",
    "\n",
    "    input_word_ids = Input((MAX_SEQUENCE_LENGTH,), name='input_word_ids',dtype=tf.int32)\n",
    "    input_masks = Input((MAX_SEQUENCE_LENGTH,), name='input_masks',dtype=tf.int32)\n",
    "    input_segments = Input((MAX_SEQUENCE_LENGTH,), name='input_segments',dtype=tf.int32)\n",
    "\n",
    "    input_num_fea =  Input((4,), name='input_num_fea')\n",
    "\n",
    "    \n",
    "    input_category = Input((1,), name='input_category')\n",
    "    input_host = Input((1,), name='input_host')\n",
    "\n",
    "    \n",
    "    num_fea_dense =  Dense(4)(input_num_fea)\n",
    "    num_fea_dense =  BatchNormalization()(num_fea_dense )\n",
    "\n",
    "    \n",
    "    category_emb = SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=6, output_dim=6)(input_category))\n",
    "    host_emb =SpatialDropout1D(0.1)(\n",
    "        Embedding(input_dim=65, output_dim=32)(input_host))\n",
    "    features_dense = concatenate([Flatten()(category_emb), Flatten()(host_emb)])\n",
    "    features_dense = Flatten()(features_dense)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    config = RobertaConfig().from_pretrained(ROBERTA_PATH+'roberta-large-config.json')\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    roberta = TFRobertaModel.from_pretrained(ROBERTA_PATH+'roberta-large-tf_model.h5', config=config)\n",
    "\n",
    "\n",
    "    for l in roberta.layers:\n",
    "        if bert_trainabel==False:\n",
    "            l.trainable = False\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    bert_output_a, pooled_output_a, hidden_output_a = roberta(input_word_ids, attention_mask=input_masks,token_type_ids=input_segments)\n",
    "    pooled_a = bert_output_a[:, -1]\n",
    "    pooled_a= Dense(768)(pooled_a)\n",
    "        \n",
    "    pooled_b = hidden_output_a[-2][:, 0]\n",
    "    pooled_b=Dense(768)(pooled_b)\n",
    "        \n",
    "    pooled_c = hidden_output_a[-2][:, -1]\n",
    "    pooled_c =Dense(768)(pooled_c)\n",
    " \n",
    "    pooled_d = hidden_output_a[-4][:, 0]\n",
    "    pooled_d =Dense(768)(pooled_d)\n",
    "        \n",
    "        \n",
    "    pooled_e = hidden_output_a[-5][:, 0]\n",
    "    pooled_e =Dense(768)(pooled_e)\n",
    "        \n",
    "    pooled_f = hidden_output_a[-6][:, 0]\n",
    "    pooled_f =Dense(768)(pooled_f)\n",
    "        \n",
    "    pooled_g = hidden_output_a[0][:, 0]\n",
    "    pooled_g=Dense(768)(pooled_g)\n",
    "        \n",
    "        \n",
    "    dense = concatenate([pooled_a,pooled_b,pooled_c,pooled_d,\\\n",
    "                         pooled_e,pooled_f,pooled_g,\\\n",
    "                         features_dense,num_fea_dense])\n",
    "\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    out = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(len(a_col))]\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_masks, input_segments, input_num_fea,input_category, input_host],\n",
    "                                  outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "for i in a_col:\n",
    "    roberta_large_oof_test[i]=0\n",
    "    roberta_large_oof_train[i]=0\n",
    "    \n",
    "if OFFLINE==False:\n",
    "\n",
    "    model_path = '/kaggle/input/dy-qa-robertalarge-v5/answer_robert_large_weights/answer_robert_large_weights'\n",
    "    \n",
    "else:\n",
    "    model_path = 'answer_robert_large_weights'\n",
    "   \n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body) \n",
    "idx = [x for x in gkf]\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(idx):\n",
    "    print('fold_: ', fold_)\n",
    "    K.clear_session()\n",
    "  \n",
    "    if OFFLINE:\n",
    "        data_train = df_train.iloc[trn_idx]\n",
    "        data_valid = df_train.iloc[val_idx]\n",
    "        print(data_train.shape)\n",
    "\n",
    "        X_train,y_train = get_input(data_train,is_train=True)\n",
    "        X_valid,y_valid = get_input(data_valid,is_train=True)\n",
    "        y_valid = compute_output_arrays2(data_valid,a_col)\n",
    "\n",
    "    model_weight = f'{model_path}/robert_fold{fold_}.h5'\n",
    "    model = create_model_a(False)\n",
    "\n",
    "    model.load_weights(model_weight)\n",
    "    \n",
    "    if OFFLINE:\n",
    "\n",
    "        valid_pred = np.array(model.predict(X_valid,batch_size=30)).squeeze().T  \n",
    "        slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
    "                                   for ind in range(valid_pred.shape[1])])\n",
    "        print('fold{} score: {}'.format(fold_, slice_score))\n",
    "\n",
    "        score.append(slice_score)\n",
    "        roberta_large_oof_train.loc[val_idx, a_col]= valid_pred\n",
    "\n",
    "    test_pred = np.array(model.predict(test_inputs,batch_size=30)).squeeze().T\n",
    "    roberta_large_oof_test.loc[:,a_col] += test_pred/5\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.953662</td>\n",
       "      <td>0.758648</td>\n",
       "      <td>0.165297</td>\n",
       "      <td>0.659731</td>\n",
       "      <td>0.784384</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.690151</td>\n",
       "      <td>0.640632</td>\n",
       "      <td>0.450913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957608</td>\n",
       "      <td>0.948003</td>\n",
       "      <td>0.627052</td>\n",
       "      <td>0.975508</td>\n",
       "      <td>0.976251</td>\n",
       "      <td>0.882877</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>0.760156</td>\n",
       "      <td>0.936132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.860280</td>\n",
       "      <td>0.454949</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.796880</td>\n",
       "      <td>0.820795</td>\n",
       "      <td>0.937832</td>\n",
       "      <td>0.553196</td>\n",
       "      <td>0.434173</td>\n",
       "      <td>0.281457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639925</td>\n",
       "      <td>0.957628</td>\n",
       "      <td>0.671719</td>\n",
       "      <td>0.974466</td>\n",
       "      <td>0.986512</td>\n",
       "      <td>0.897611</td>\n",
       "      <td>0.929819</td>\n",
       "      <td>0.114369</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.917170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.939485</td>\n",
       "      <td>0.736497</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.827498</td>\n",
       "      <td>0.933696</td>\n",
       "      <td>0.955366</td>\n",
       "      <td>0.602811</td>\n",
       "      <td>0.538691</td>\n",
       "      <td>0.118131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926948</td>\n",
       "      <td>0.948273</td>\n",
       "      <td>0.603985</td>\n",
       "      <td>0.970868</td>\n",
       "      <td>0.973403</td>\n",
       "      <td>0.859570</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>0.881194</td>\n",
       "      <td>0.925790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.884104</td>\n",
       "      <td>0.482127</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.682158</td>\n",
       "      <td>0.779390</td>\n",
       "      <td>0.919421</td>\n",
       "      <td>0.543002</td>\n",
       "      <td>0.346256</td>\n",
       "      <td>0.052227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717085</td>\n",
       "      <td>0.963253</td>\n",
       "      <td>0.702174</td>\n",
       "      <td>0.978615</td>\n",
       "      <td>0.988025</td>\n",
       "      <td>0.905726</td>\n",
       "      <td>0.839684</td>\n",
       "      <td>0.104251</td>\n",
       "      <td>0.755389</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.935540</td>\n",
       "      <td>0.544810</td>\n",
       "      <td>0.028294</td>\n",
       "      <td>0.839703</td>\n",
       "      <td>0.857227</td>\n",
       "      <td>0.867264</td>\n",
       "      <td>0.655872</td>\n",
       "      <td>0.657910</td>\n",
       "      <td>0.171043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832703</td>\n",
       "      <td>0.948149</td>\n",
       "      <td>0.698295</td>\n",
       "      <td>0.975982</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.900220</td>\n",
       "      <td>0.484976</td>\n",
       "      <td>0.183508</td>\n",
       "      <td>0.382474</td>\n",
       "      <td>0.935529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.953662                0.758648   \n",
       "1     46                             0.860280                0.454949   \n",
       "2     70                             0.939485                0.736497   \n",
       "3    132                             0.884104                0.482127   \n",
       "4    200                             0.935540                0.544810   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.165297                      0.659731   \n",
       "1                 0.003170                      0.796880   \n",
       "2                 0.011118                      0.827498   \n",
       "3                 0.003183                      0.682158   \n",
       "4                 0.028294                      0.839703   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.784384                               0.682129   \n",
       "1               0.820795                               0.937832   \n",
       "2               0.933696                               0.955366   \n",
       "3               0.779390                               0.919421   \n",
       "4               0.857227                               0.867264   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.690151                       0.640632   \n",
       "1                         0.553196                       0.434173   \n",
       "2                         0.602811                       0.538691   \n",
       "3                         0.543002                       0.346256   \n",
       "4                         0.655872                       0.657910   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.450913  ...               0.957608        0.948003   \n",
       "1               0.281457  ...               0.639925        0.957628   \n",
       "2               0.118131  ...               0.926948        0.948273   \n",
       "3               0.052227  ...               0.717085        0.963253   \n",
       "4               0.171043  ...               0.832703        0.948149   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.627052          0.975508          0.976251   \n",
       "1                     0.671719          0.974466          0.986512   \n",
       "2                     0.603985          0.970868          0.973403   \n",
       "3                     0.702174          0.978615          0.988025   \n",
       "4                     0.698295          0.975982          0.974700   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.882877                  0.013333               0.016103   \n",
       "1             0.897611                  0.929819               0.114369   \n",
       "2             0.859570                  0.020221               0.021124   \n",
       "3             0.905726                  0.839684               0.104251   \n",
       "4             0.900220                  0.484976               0.183508   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.760156             0.936132  \n",
       "1                        0.053662             0.917170  \n",
       "2                        0.881194             0.925790  \n",
       "3                        0.755389             0.922300  \n",
       "4                        0.382474             0.935529  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_large_oof_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OFFLINE:\n",
    "\n",
    "    def compute_spearmanr_ignore_nan(trues, preds):\n",
    "        rhos = []\n",
    "        for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "            rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "        return np.nanmean(rhos)\n",
    "\n",
    "\n",
    "    print(compute_spearmanr_ignore_nan(roberta_large_oof_train[targets].values,df_train[targets].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OFFLINE:\n",
    "\n",
    "    blend =roberta_base_oof_train456[targets].values*0.4+roberta_large_oof_train[targets].values*0.4+bera_base_oof_train[targets].values*0.2\n",
    "\n",
    "    print(compute_spearmanr_ignore_nan(blend,df_train[targets].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = roberta_large_oof_test.copy()\n",
    "blend.loc[:,targets] = roberta_base_oof_test456.loc[:,targets]*0.4+\\\n",
    "                roberta_large_oof_test.loc[:,targets]*0.4+bert_base_oof_test.loc[:,targets]*0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>0.739088</td>\n",
       "      <td>0.159843</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>0.756393</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>0.698231</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.482985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954519</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.611972</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>0.973664</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.012994</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.792141</td>\n",
       "      <td>0.930814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.870538</td>\n",
       "      <td>0.482489</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.790949</td>\n",
       "      <td>0.809230</td>\n",
       "      <td>0.935954</td>\n",
       "      <td>0.559164</td>\n",
       "      <td>0.453499</td>\n",
       "      <td>0.255959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654642</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.666872</td>\n",
       "      <td>0.974245</td>\n",
       "      <td>0.986437</td>\n",
       "      <td>0.896125</td>\n",
       "      <td>0.926248</td>\n",
       "      <td>0.116002</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.919107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.940564</td>\n",
       "      <td>0.725528</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.938285</td>\n",
       "      <td>0.954063</td>\n",
       "      <td>0.605832</td>\n",
       "      <td>0.519423</td>\n",
       "      <td>0.118501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922801</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.605793</td>\n",
       "      <td>0.974276</td>\n",
       "      <td>0.975519</td>\n",
       "      <td>0.865711</td>\n",
       "      <td>0.027659</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.888995</td>\n",
       "      <td>0.925022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.897449</td>\n",
       "      <td>0.492817</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.726167</td>\n",
       "      <td>0.761701</td>\n",
       "      <td>0.917681</td>\n",
       "      <td>0.552307</td>\n",
       "      <td>0.393249</td>\n",
       "      <td>0.049131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745199</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>0.691679</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.988562</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>0.866322</td>\n",
       "      <td>0.136860</td>\n",
       "      <td>0.767863</td>\n",
       "      <td>0.923064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.935290</td>\n",
       "      <td>0.516668</td>\n",
       "      <td>0.021014</td>\n",
       "      <td>0.848693</td>\n",
       "      <td>0.862901</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.660954</td>\n",
       "      <td>0.632707</td>\n",
       "      <td>0.147777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800392</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.677849</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.969091</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.155721</td>\n",
       "      <td>0.444974</td>\n",
       "      <td>0.926133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.954014                0.739088   \n",
       "1     46                             0.870538                0.482489   \n",
       "2     70                             0.940564                0.725528   \n",
       "3    132                             0.897449                0.492817   \n",
       "4    200                             0.935290                0.516668   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.159843                      0.615849   \n",
       "1                 0.003683                      0.790949   \n",
       "2                 0.011299                      0.833017   \n",
       "3                 0.003731                      0.726167   \n",
       "4                 0.021014                      0.848693   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.756393                               0.616029   \n",
       "1               0.809230                               0.935954   \n",
       "2               0.938285                               0.954063   \n",
       "3               0.761701                               0.917681   \n",
       "4               0.862901                               0.898000   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.698231                       0.650771   \n",
       "1                         0.559164                       0.453499   \n",
       "2                         0.605832                       0.519423   \n",
       "3                         0.552307                       0.393249   \n",
       "4                         0.660954                       0.632707   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.482985  ...               0.954519        0.938095   \n",
       "1               0.255959  ...               0.654642        0.957379   \n",
       "2               0.118501  ...               0.922801        0.946523   \n",
       "3               0.049131  ...               0.745199        0.963407   \n",
       "4               0.147777  ...               0.800392        0.930432   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.611972          0.970559          0.973664   \n",
       "1                     0.666872          0.974245          0.986437   \n",
       "2                     0.605793          0.974276          0.975519   \n",
       "3                     0.691679          0.978456          0.988562   \n",
       "4                     0.677849          0.970682          0.969091   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.868776                  0.012994               0.017060   \n",
       "1             0.896125                  0.926248               0.116002   \n",
       "2             0.865711                  0.027659               0.025179   \n",
       "3             0.895506                  0.866322               0.136860   \n",
       "4             0.871728                  0.369760               0.155721   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.792141             0.930814  \n",
       "1                        0.056720             0.919107  \n",
       "2                        0.888995             0.925022  \n",
       "3                        0.767863             0.923064  \n",
       "4                        0.444974             0.926133  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>pct</th>\n",
       "      <th>choice</th>\n",
       "      <th>score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question_asker_intent_understanding</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>low</td>\n",
       "      <td>0.406266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question_body_critical</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>low</td>\n",
       "      <td>0.656163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question_conversational</td>\n",
       "      <td>0.888633</td>\n",
       "      <td>low</td>\n",
       "      <td>0.538601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>question_expect_short_answer</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>up</td>\n",
       "      <td>0.351612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question_fact_seeking</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>up</td>\n",
       "      <td>0.402604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   col       pct choice    score2\n",
       "0  question_asker_intent_understanding  0.000165    low  0.406266\n",
       "1               question_body_critical  0.012667    low  0.656163\n",
       "2              question_conversational  0.888633    low  0.538601\n",
       "3         question_expect_short_answer  0.035861     up  0.351612\n",
       "4                question_fact_seeking  0.066787     up  0.402604"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dict = {'col': ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written'], 'pct': [0.000165, 0.012667, 0.888633, 0.035861, 0.066787, 0.74634, 0.035861, 0.583978, 0.597631, 0.969238, 0.041619, 0.467676, 0.928442, 0.962823, 0.928442, 0.876131, 0.377858, 0.099194, 0.235072, 0.962823, 0.080112, 0.004935, 0.002797, 0.175193, 0.619674, 0.002797, 0.327192, 0.002797, 0.12354, 0.327192], 'choice': ['low', 'low', 'low', 'up', 'up', 'up', 'up', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'up', 'low', 'low', 'low', 'up', 'low', 'up', 'up', 'up', 'up', 'up', 'up', 'low', 'up'], 'score2': [0.406266, 0.656163, 0.538601, 0.351612, 0.402604, 0.516523, 0.379025, 0.53059, 0.659577, 0.193691, 0.52533, 0.798482, 0.591538, 0.351739, 0.668533, 0.644524, 0.809915, 0.39668, 0.713513, 0.173615, 0.520332, 0.304857, 0.460612, 0.202765, 0.216578, 0.384599, 0.775576, 0.322843, 0.712331, 0.259617]}\n",
    "\n",
    "\n",
    "\n",
    "hyper = pd.DataFrame(hyper_dict)\n",
    "hyper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "\n",
    "for col in targets:\n",
    "    if hyper.loc[hyper.col==col, 'choice'].values == 'low':\n",
    "        if hyper.loc[hyper.col==col, 'pct'].values == 1:\n",
    "            pct = hyper.loc[hyper.col==col, 'pct'] #-0.01\n",
    "        else:\n",
    "            pct  = hyper.loc[hyper.col==col, 'pct']\n",
    "            \n",
    "        changerow = int(len(df_test) * pct)\n",
    "        rowidx = blend.loc[:, col].argsort()[:changerow]\n",
    "        blend.loc[rowidx, col] = 0\n",
    "\n",
    "    elif hyper.loc[hyper.col==col, 'choice'].values == 'up':\n",
    "        if hyper.loc[hyper.col==col, 'pct'].values == 1:\n",
    "            pct = hyper.loc[hyper.col==col, 'pct'] #-0.01\n",
    "        else:\n",
    "            pct  = hyper.loc[hyper.col==col, 'pct']\n",
    "            \n",
    "        changerow = int(len(df_test) * pct)\n",
    "        rowidx = blend.loc[:, col].argsort()[-changerow:]\n",
    "        blend.loc[rowidx, col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols2rd ={'question_asker_intent_understanding': 4,\n",
    "#  'question_body_critical': 7,\n",
    "#  'question_conversational': 3,\n",
    "#  'question_expect_short_answer': 4,\n",
    "#  'question_fact_seeking': 4,\n",
    "#  'question_has_commonly_accepted_answer': 3,\n",
    "#  'question_interestingness_others': 2,\n",
    "#  'question_interestingness_self': 2,\n",
    "#  'question_multi_intent': 3,\n",
    "#  'question_not_really_a_question': 3,\n",
    "#  'question_opinion_seeking': 4,\n",
    "#  'question_type_choice': 6,\n",
    "#  'question_type_compare': 2,\n",
    "#  'question_type_consequence': 5,\n",
    "#  'question_type_definition': 2,\n",
    "#  'question_type_entity': 5,\n",
    "#  'question_type_instructions': 1,\n",
    "#  'question_type_procedure': 3,\n",
    "#  'question_type_reason_explanation': 4,\n",
    "#  'question_type_spelling': 3,\n",
    "#  'question_well_written': 6,\n",
    "#  'answer_helpful': 4,\n",
    "#  'answer_level_of_information': 3,\n",
    "#  'answer_plausible': 3,\n",
    "#  'answer_relevance': 2,\n",
    "#  'answer_satisfaction': 3,\n",
    "#  'answer_type_instructions': 2,\n",
    "#  'answer_type_procedure': 3,\n",
    "#  'answer_type_reason_explanation': 2,\n",
    "#  'answer_well_written': 3}\n",
    "\n",
    "# for i in targets:\n",
    "#     v = blend[i].values\n",
    "#     if \n",
    "#     blend.loc[:,i] = np.round(v,cols2rd[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>0.739088</td>\n",
       "      <td>0.159843</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>0.756393</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>0.698231</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.482985</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.611972</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>0.973664</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.012994</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.792141</td>\n",
       "      <td>0.930814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.870538</td>\n",
       "      <td>0.482489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790949</td>\n",
       "      <td>0.809230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.559164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654642</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.666872</td>\n",
       "      <td>0.974245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.940564</td>\n",
       "      <td>0.725528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.938285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605832</td>\n",
       "      <td>0.519423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922801</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.605793</td>\n",
       "      <td>0.974276</td>\n",
       "      <td>0.975519</td>\n",
       "      <td>0.865711</td>\n",
       "      <td>0.027659</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.888995</td>\n",
       "      <td>0.925022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.897449</td>\n",
       "      <td>0.492817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726167</td>\n",
       "      <td>0.761701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.552307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745199</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>0.691679</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136860</td>\n",
       "      <td>0.767863</td>\n",
       "      <td>0.923064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.935290</td>\n",
       "      <td>0.516668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848693</td>\n",
       "      <td>0.862901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660954</td>\n",
       "      <td>0.632707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800392</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.677849</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.969091</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.155721</td>\n",
       "      <td>0.444974</td>\n",
       "      <td>0.926133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.954014                0.739088   \n",
       "1     46                             0.870538                0.482489   \n",
       "2     70                             0.940564                0.725528   \n",
       "3    132                             0.897449                0.492817   \n",
       "4    200                             0.935290                0.516668   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.159843                      0.615849   \n",
       "1                 0.000000                      0.790949   \n",
       "2                 0.000000                      0.833017   \n",
       "3                 0.000000                      0.726167   \n",
       "4                 0.000000                      0.848693   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.756393                               0.616029   \n",
       "1               0.809230                               1.000000   \n",
       "2               0.938285                               1.000000   \n",
       "3               0.761701                               1.000000   \n",
       "4               0.862901                               1.000000   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.698231                       0.650771   \n",
       "1                         0.559164                       0.000000   \n",
       "2                         0.605832                       0.519423   \n",
       "3                         0.552307                       0.000000   \n",
       "4                         0.660954                       0.632707   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.482985  ...               1.000000        0.938095   \n",
       "1               0.255959  ...               0.654642        0.957379   \n",
       "2               0.000000  ...               0.922801        0.946523   \n",
       "3               0.000000  ...               0.745199        0.963407   \n",
       "4               0.000000  ...               0.800392        0.930432   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.611972          0.970559          0.973664   \n",
       "1                     0.666872          0.974245          1.000000   \n",
       "2                     0.605793          0.974276          0.975519   \n",
       "3                     0.691679          0.978456          1.000000   \n",
       "4                     0.677849          0.970682          0.969091   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.868776                  0.012994               0.017060   \n",
       "1             0.896125                  1.000000               0.116002   \n",
       "2             0.865711                  0.027659               0.025179   \n",
       "3             0.895506                  1.000000               0.136860   \n",
       "4             0.871728                  0.369760               0.155721   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.792141             0.930814  \n",
       "1                        0.000000             0.919107  \n",
       "2                        0.888995             0.925022  \n",
       "3                        0.767863             0.923064  \n",
       "4                        0.444974             0.926133  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2posrate = {col:(df_train[col]>0).mean() for col in targets}\n",
    "col2npos = {col:int(len(df_test)*posrate) for col,posrate in col2posrate.items()}\n",
    "col2npos = {col:max(1,npos) for col,npos in col2npos.items()}\n",
    "col2npos = {col:min(len(df_test)-1, npos) for col,npos in col2npos.items()}\n",
    "\n",
    "col2prd_top_perc = {}\n",
    "for i,col in enumerate(targets):\n",
    "    prdi = blend.loc[:,col].values\n",
    "    prd_top_perc = np.zeros(prdi.shape)\n",
    "    npos=col2npos[col]\n",
    "    idx_sorted=(-prdi).argsort()\n",
    "    idx_pos=idx_sorted[:npos]\n",
    "    prd_top_perc[idx_pos]=1\n",
    "    col2prd_top_perc[col]=prd_top_perc\n",
    "\n",
    "def pstpst(sub):\n",
    "    for col in sub.columns[1:]:\n",
    "        if len(set(sub[col]))>1:continue\n",
    "        print(col, 'has only 1 value!')\n",
    "        sub[col]=col2prd_top_perc[col]\n",
    "        print(col, f'now has {len(set(sub[col]))} value!')\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = pstpst(blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>0.739088</td>\n",
       "      <td>0.159843</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>0.756393</td>\n",
       "      <td>0.616029</td>\n",
       "      <td>0.698231</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.482985</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.611972</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>0.973664</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.012994</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.792141</td>\n",
       "      <td>0.930814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.870538</td>\n",
       "      <td>0.482489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790949</td>\n",
       "      <td>0.809230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.559164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654642</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.666872</td>\n",
       "      <td>0.974245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.940564</td>\n",
       "      <td>0.725528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833017</td>\n",
       "      <td>0.938285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605832</td>\n",
       "      <td>0.519423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922801</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.605793</td>\n",
       "      <td>0.974276</td>\n",
       "      <td>0.975519</td>\n",
       "      <td>0.865711</td>\n",
       "      <td>0.027659</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.888995</td>\n",
       "      <td>0.925022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.897449</td>\n",
       "      <td>0.492817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726167</td>\n",
       "      <td>0.761701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.552307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745199</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>0.691679</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136860</td>\n",
       "      <td>0.767863</td>\n",
       "      <td>0.923064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.935290</td>\n",
       "      <td>0.516668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848693</td>\n",
       "      <td>0.862901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660954</td>\n",
       "      <td>0.632707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800392</td>\n",
       "      <td>0.930432</td>\n",
       "      <td>0.677849</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.969091</td>\n",
       "      <td>0.871728</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.155721</td>\n",
       "      <td>0.444974</td>\n",
       "      <td>0.926133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.954014                0.739088   \n",
       "1     46                             0.870538                0.482489   \n",
       "2     70                             0.940564                0.725528   \n",
       "3    132                             0.897449                0.492817   \n",
       "4    200                             0.935290                0.516668   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.159843                      0.615849   \n",
       "1                 0.000000                      0.790949   \n",
       "2                 0.000000                      0.833017   \n",
       "3                 0.000000                      0.726167   \n",
       "4                 0.000000                      0.848693   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.756393                               0.616029   \n",
       "1               0.809230                               1.000000   \n",
       "2               0.938285                               1.000000   \n",
       "3               0.761701                               1.000000   \n",
       "4               0.862901                               1.000000   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.698231                       0.650771   \n",
       "1                         0.559164                       0.000000   \n",
       "2                         0.605832                       0.519423   \n",
       "3                         0.552307                       0.000000   \n",
       "4                         0.660954                       0.632707   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.482985  ...               1.000000        0.938095   \n",
       "1               0.255959  ...               0.654642        0.957379   \n",
       "2               0.000000  ...               0.922801        0.946523   \n",
       "3               0.000000  ...               0.745199        0.963407   \n",
       "4               0.000000  ...               0.800392        0.930432   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.611972          0.970559          0.973664   \n",
       "1                     0.666872          0.974245          1.000000   \n",
       "2                     0.605793          0.974276          0.975519   \n",
       "3                     0.691679          0.978456          1.000000   \n",
       "4                     0.677849          0.970682          0.969091   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.868776                  0.012994               0.017060   \n",
       "1             0.896125                  1.000000               0.116002   \n",
       "2             0.865711                  0.027659               0.025179   \n",
       "3             0.895506                  1.000000               0.136860   \n",
       "4             0.871728                  0.369760               0.155721   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.792141             0.930814  \n",
       "1                        0.000000             0.919107  \n",
       "2                        0.888995             0.925022  \n",
       "3                        0.767863             0.923064  \n",
       "4                        0.444974             0.926133  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule 1\n",
    "n=df_test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\n",
    "spelling=[]\n",
    "for x in n:\n",
    "    if x:\n",
    "        spelling.append(0.5)\n",
    "    else:\n",
    "        spelling.append(0.)\n",
    "        \n",
    "blend['question_type_spelling']=spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 31)\n"
     ]
    }
   ],
   "source": [
    "print(blend.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
